[{"content":"I believe that when more people understand technology, companies providing technology services will be held to a higher standard. So, in service of that, here\u0026rsquo;s my pitch to get you to start a homelab!\nWhat is a Homelab? At its core, a homelab is a safe place to experiment. With hardware, software, networking, and even dealing with angry users when your recipes app isn\u0026rsquo;t accessible and your significant other really needs the next step for that chili!\nIt can start out with totally 0 budget by using something like an old laptop, which is a homelab hack if ever I\u0026rsquo;ve seen one. You\u0026rsquo;ve likely already got it, it has a build in UPS (called a \u0026lsquo;battery\u0026rsquo;), a built in KVM (\u0026lsquo;screen\u0026rsquo; and \u0026lsquo;keyboard\u0026rsquo;) and if it\u0026rsquo;s actually an old laptop, a built in ethernet port. If not, a USB-to-Ethernet adapter will cost you as much as a cup of coffee so don\u0026rsquo;t freak out too much.\nLearning Through Doing One of the primary benefits of a homelab is hands-on learning. Reading documentation and tutorials is valuable, but you really need a place to follow along and cement that knowledge. Without that, you\u0026rsquo;re missing the 80% for the 20%.\nMy homelab has been an invaluable classroom where I\u0026rsquo;ve:\nDiscovered that YAML indentation errors are the bane of my existence Learned why proper backups are essential after accidentally wiping a drive Repeatedly assured myself that this time I\u0026rsquo;d document everything properly (still \u0026ldquo;WIP\u0026rdquo; if we\u0026rsquo;re being honest ðŸ˜…) All of these are things you definitely want to do to yourself before you do them to your FTSE100 Indexed employer. Just head to Google News and search for \u0026lsquo;misconfiguration\u0026rsquo; or \u0026lsquo;glitch\u0026rsquo; and you\u0026rsquo;ll see folks who would\u0026rsquo;ve benefited from a homelab.\nProfessional Growth If you\u0026rsquo;re in the ðŸŒŸ tech world ðŸŒŸ at all, you know you have to keep your skills sharp. You might spend months on a project related to a niche skill you just happened to have from your uni days and then start to lose your muscle memory for working with more modern tech. This seems like an oddly specific point to make but it\u0026rsquo;s happened to me multiple times, in multiple different jobs.\nOther than keeping your skills sharp, a homelab works as a public demonstration of your workmanship. Either make it public, or couple it with a blog (the infrastructure of which can also become a selling point), and you\u0026rsquo;ll already have a leg up in interviews over other candidates. I\u0026rsquo;ve had multiple job interviews that have started with \u0026ldquo;so, I read xyz on your blog.\u0026rdquo;\nYou don\u0026rsquo;t necessarily need to be twitter famous or anything (I\u0026rsquo;m definitely not). Just have something linked from the website in your CV or your LinkedIn for your potential boss to land on when pre-employment stalking you.\nCreating Rather Than Consuming One of my personal favourite aspect of a homelab is the transition from consumer to creator. Once you\u0026rsquo;ve been around long enough to see Google sunset a few of your favourite services (we still love you, Google Reader ðŸ¥²), you start looking for alternatives. These days, someone with a paid substack and a NextJS app they had AI write for them will probably suggest that they can teach you how, but I can promise you that unless you want to deploy it to Vercel\u0026rsquo;s cloud you need another way.\nYou don\u0026rsquo;t have to build it from scratch either, there\u0026rsquo;s plenty of open source apps that you can spin up to replace these services, you just need a place to deploy them. That could be a $5/m VPS from someone like Linode, which is great to start out with, but you inevitably hit an issue with storage or networking or memory and then things get expensive fast. An upgrade to your homelab that you could make yourself which costs you $20 will cost you that per month with some VPS providers.\nThere is a special satisfaction about being able to say \u0026ldquo;oh yeah, I can just run that for you\u0026rdquo; when a friend complains about the cost of extra iCloud storage.\nSome Tips on Starting Your Own Homelab Journey If I\u0026rsquo;ve done my job and you\u0026rsquo;re inspired to start your own homelab journey, here are some quick points to keep in mind:\nStart small and set interesting goals that you can achieve quickly - there\u0026rsquo;ll be a future post on ideal homelab projects for beginners but for now check out Homepage, Mealie, or Linkding. Practice documenting everything to find your preferred style and have reasons for your opinion - being able to articulate these when collaborating with teams in the future makes you look like you know what you\u0026rsquo;re doing, and then everyone does things your way! Join communities like r/homelab, r/selfhosted, or The Level1 Forum and READ FIRST - the worst thing to do is go blasting away in forums where you don\u0026rsquo;t know the etiquette and your question has been solved 12 times already As you start to figure out your preferred way of doing certain things, remember the phrase \u0026ldquo;strong opinions, loosely held.\u0026rdquo; You should have reasons for your views, but if new evidence presents itself then you should be humble enough to take that on board and re-evaluate.\n*Have you started your homelab journey? Do you have \u0026lsquo;where do I go from here\u0026rsquo; type questions? I\u0026rsquo;d love to nerd out or give you encouragement, shoot me a message on Mastodon!\n","permalink":"https://dangerous.tech/the-importance-of-the-homelab/","summary":"\u003cp\u003eI believe that when more people understand technology, companies providing technology services will be held to a higher standard. So, in service of that, here\u0026rsquo;s my pitch to get you to start a homelab!\u003c/p\u003e\n\u003ch2 id=\"what-is-a-homelab\"\u003eWhat is a Homelab?\u003c/h2\u003e\n\u003cp\u003eAt its core, a homelab is a safe place to experiment. With hardware, software, networking, and even dealing with angry users when your recipes app isn\u0026rsquo;t accessible and your significant other really needs the next step for that chili!\u003c/p\u003e","title":"The Importance of the Homelab"},{"content":"This one\u0026rsquo;s a slight philsophical break.\nI\u0026rsquo;ve been on a few different journeys since the last post on this blog\u0026hellip; 2 different jobs, consulting work, and then starting my own company with a long term friend and now business partner!\nI figured I\u0026rsquo;d start out with a bit of a shower thought I had. I\u0026rsquo;d like to occasionally take this blog in a direction of more philosophical thinkery, because that\u0026rsquo;s always been another interest of mine. I was considering a hypothetical scenario whereby someone whom you\u0026rsquo;re supposed to respect and whom you want to respect (think: family-in-law, boss, government official), does something untoward. Something that could be addressed with Hanlon\u0026rsquo;s razor - \u0026ldquo;never attribute to malice that which is adequately explained by stupidity.\u0026rdquo;\nI find it hard to respect stupid people. Luckily there is some interpretive wiggle room when we determine Hanlon\u0026rsquo;s definition of \u0026ldquo;stupidity,\u0026rdquo; but how much wiggle room do we give? If these are people I want to respect, it biases me to thinking that they\u0026rsquo;re mostly malicious in these actions. Having to think that they\u0026rsquo;re a malicious person isn\u0026rsquo;t good, per se. But I\u0026rsquo;d rather think that the person was respectable yet malicious instead of stupid and, therfore, unworthy of respect.\nIt just doesn\u0026rsquo;t hold out in the long run for wanting any kind of deeper relationship with that kind of person! It would seem that in this, like many other scenarios, the anwswer lies somewhere in the realm of \u0026ldquo;communicate more.\u0026rdquo; Or if not that, then \u0026ldquo;communicate better.\u0026rdquo; Even as an engineer (read: not innately a fan of communicating), I started to notice in my career and relationships that it really is the quality of your communication that makes all the difference\u0026hellip;\n","permalink":"https://dangerous.tech/on-attributing-to-malice/","summary":"\u003cp\u003eThis one\u0026rsquo;s a slight philsophical break.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve been on a few different journeys since the last post on this blog\u0026hellip; 2 different jobs, consulting work, and then starting my own company with a long term friend and now business partner!\u003c/p\u003e\n\u003cp\u003eI figured I\u0026rsquo;d start out with a bit of a shower thought I had. I\u0026rsquo;d like to occasionally take this blog in a direction of more philosophical thinkery, because that\u0026rsquo;s always been another interest of mine. I was considering a hypothetical scenario whereby someone whom you\u0026rsquo;re \u003cem\u003esupposed\u003c/em\u003e to respect and whom you \u003cem\u003ewant\u003c/em\u003e to respect (think: family-in-law, boss, government official), does something untoward. Something that could be addressed with Hanlon\u0026rsquo;s razor - \u0026ldquo;never attribute to malice that which is adequately explained by stupidity.\u0026rdquo;\u003c/p\u003e","title":"On Attributing To Malice"},{"content":"Within an article on their website, LinuxServer.io have announced that they will be deprecating the current linuxserver/unifi-controller public image as of 2024/01/01. Happy New Year! The README on the current tooling that I imagine most are using mentions a direct upgrade if you were using the mongoless tag which, due to what\u0026rsquo;s colloquially known as \u0026ldquo;Sod\u0026rsquo;s Law\u0026rdquo;, I am not.\nOn the new repo as well as the DockerHub page, the docs currently have a brief mentioning of requiring an external MongoDB instance but the code snippits don\u0026rsquo;t actuall have one\u0026hellip; This seemed a little problematic, and after migrating my setup last night, figured I\u0026rsquo;d include a quick walkthrough.\nThe Solution Unfortunately, there\u0026rsquo;s no one and done migration to just switch images and bring the container back up, but it\u0026rsquo;s not exactly a difficult process.\nI\u0026rsquo;ve included the docker-compose.yml in the section below, just copy pasta, change the passwords (anywhere you see SUPERSTRONGPASSWORD), and you\u0026rsquo;re good (also create a init-mongo.js using the below file too as this will automatically create the correct user, db, and permissions).\nDO NOT copy across the current Unifi config folder, restoring from the backup will create this in a safer way.\nThe newest supported MongoDB version currently is 4.4 so that\u0026rsquo;s what I\u0026rsquo;m using in my stack, if that changes I\u0026rsquo;ll likely see about migrating to newer, and will update this post if I do.\nManual backup of current configuration\nSettings \u0026gt; System \u0026gt; Backups \u0026gt; Download (in the new UI, anyway) I\u0026rsquo;d set the export time to \u0026lsquo;No Limit\u0026rsquo; to keep stats, etc, but you do you Stop the current container\nJust do a docker compose down on the current stack Unless you\u0026rsquo;re migrating to another box you need to stop the current stack because the new one is going to need the same ports :upside_down_face: Start the new container\nI said it above, but just in case, copy the compose file into the new location (I\u0026rsquo;d just make a different folder for this new controller) You guessed it, docker compose up -d I\u0026rsquo;d recommend watching the logs with docker compose logs -f because you\u0026rsquo;ll see a final log line from the MongoDB container related to Authentication, and as long as that\u0026rsquo;s not an error that\u0026rsquo;s when you\u0026rsquo;ll know that the container is up and ready Restore from the backup\nGo to https://container-host-name-or-ip:8443 Start the process, until you see a link allowing you to resore from a backup Click that link Select the backup you just downloaded from the old controller Wait a couple of minues, you\u0026rsquo;ll see the MongoDB logs going crazy and then the controller will reboot Reload, log in, and check all your APs still report in Breathe a sigh of relief You can now remove the volume binding for the init-mongo.js file and restart the stack for purity\u0026rsquo;s sake if you like Don\u0026rsquo;t touch it for another 6 months :wink:\nAnd the best bit is that if it breaks in some way and you need to roll back, just start the old stack and you\u0026rsquo;re back in business :partying_face:.\nDocker-Compose File NB: You can also use a proper Docker volume instead of a local folder in the unifi-db container\nversion: \u0026#34;2.1\u0026#34; services: unifi-db: image: mongo:4.4 restart: always volumes: - ./db:/data/db - ./init-mongo.js:/docker-entrypoint-initdb.d/init-mongo.js:ro unifi-network-application: image: lscr.io/linuxserver/unifi-network-application:latest container_name: unifi-network-application environment: - PUID=1000 - PGID=1000 - TZ=Europe/London - MONGO_USER=unifi - MONGO_PASS=SUPERSTRONGPASSWORD - MONGO_HOST=unifi-db - MONGO_PORT=27017 - MONGO_DBNAME=unifi - MEM_LIMIT=1024 #optional - MEM_STARTUP=1024 #optional volumes: - ./config:/config ports: - 8443:8443 - 3478:3478/udp - 10001:10001/udp - 8080:8080 - 8843:8843 #optional - 8880:8880 #optional - 6789:6789 #optional - 5514:5514/udp #optional restart: always depends_on: - unifi-db init-mongo.js File db.getSiblingDB(\u0026#34;unifi\u0026#34;).createUser({ user: \u0026#34;unifi\u0026#34;, pwd: \u0026#34;SUPERSTRONGPASSWORD\u0026#34;, roles: [{ role: \u0026#34;readWrite\u0026#34;, db: \u0026#34;unifi\u0026#34; }], }); db.getSiblingDB(\u0026#34;unifi_stat\u0026#34;).createUser({ user: \u0026#34;unifi\u0026#34;, pwd: \u0026#34;SUPERSTRONGPASSWORD\u0026#34;, roles: [{ role: \u0026#34;readWrite\u0026#34;, db: \u0026#34;unifi_stat\u0026#34; }], }); ","permalink":"https://dangerous.tech/linuxserver.io-unifi-controller-deprecation/","summary":"\u003cp\u003eWithin an article on \u003ca href=\"https://info.linuxserver.io/issues/2023-09-06-unifi-controller/\"\u003etheir website\u003c/a\u003e, LinuxServer.io have announced that they will be deprecating the current \u003ccode\u003elinuxserver/unifi-controller\u003c/code\u003e public image as of 2024/01/01. Happy New Year! The README on the current tooling that I imagine most are using mentions a direct upgrade if you were using the \u003ccode\u003emongoless\u003c/code\u003e tag which, due to what\u0026rsquo;s colloquially known as \u0026ldquo;Sod\u0026rsquo;s Law\u0026rdquo;, I am not.\u003c/p\u003e\n\u003cp\u003eOn the new \u003ca href=\"https://github.com/linuxserver/docker-unifi-network-application\"\u003erepo\u003c/a\u003e as well as the \u003ca href=\"https://hub.docker.com/r/linuxserver/unifi-network-application\"\u003eDockerHub page\u003c/a\u003e, the docs currently have a brief mentioning of requiring an external MongoDB instance but the code snippits don\u0026rsquo;t actuall have one\u0026hellip; This seemed a little problematic, and after migrating my setup last night, figured I\u0026rsquo;d include a quick walkthrough.\u003c/p\u003e","title":"LinuxServer.IO Unifi Controller Deprecation"},{"content":"In my previous post, I detailed moving your Traefik configuration to a slightly more permanent format (YAML\u0026hellip; I know, just go with it). You might have then been tempted to go to SSL Labs and do an SSL Test. What you would have discovered, in that tragic scenario, is a mere B grade!\n\u0026ldquo;Why, oh why?\u0026rdquo; I hear you cry, \u0026ldquo;I did everything right!\u0026rdquo; Well, technically you did, so don\u0026rsquo;t feel too bad. It\u0026rsquo;s not a fault of yours that you\u0026rsquo;re letting people with filthy TLS 1.0 and 1.1 view your site. It is, however, the view of certain industry bodies that these technologies are deprecated and therefore have multiple nasty security vulnerabilities.\nCaveats You\u0026rsquo;re running Traefik in a container, bonus points if you\u0026rsquo;re using Docker Compose You\u0026rsquo;ve backed up your current running config and volume contents You understand basic Docker concepts like bringing containers up and down, and what that will do to your currently running infrastructure A Quick Recap Now, if you\u0026rsquo;d gone through that previous post I mentioned above, you would have read about the difference between Static and Dynamic configuration in Traefik. A quick reminder:\nStatic runs at startup Dynamic runs each time a new request comes in to Traefik So with Static configuration, we can define the entrypoints upon which Traefik accepts traffic (port 80, 443, etc) but according to Traefik\u0026rsquo;s split of these rules, defining which TLS version is accepted is done Dynamically, i.e. checked every time a new connection comes in to the router. \u0026ldquo;But\u0026hellip; but\u0026hellip;\u0026rdquo; I hear you stammer, \u0026ldquo;I\u0026rsquo;ve only passed through a static configuration file! How do?\u0026rdquo;\nIt really is just as easy as the static configuration file, with one small addition.\nLoading The File To enable us to test this method as we go, we need to create an empty dynamic.yml file in the same directory as the rest of our configuration. Once this is done, we need to add it to our traefik.yml file (or whatever you\u0026rsquo;ve called your static config file) as a \u0026lsquo;Provider.\u0026rsquo; You\u0026rsquo;ll probably also have something like Docker in here too, see the bottom of the post for my full file in context.\nproviders: file: filename: /etc/traefik/dynamic.yml The final step is to add this to our docker-compose.yml file so that the container has access to this file. Again, see the bottom of the post for my full file.\nvolumes: - \u0026#34;./dynamic.yml:/etc/traefik/dynamic.yml\u0026#34; Now, we\u0026rsquo;re ready to add some config to dynamic.yml file!\nThe Dynamic File It\u0026rsquo;s worth mentioning here that I haven\u0026rsquo;t yet seen a successful way to do this via the cmdline and trust me, I\u0026rsquo;ve looked\u0026hellip; This was the impetus for me converting my static config to a YAML file in the first place, so just trust me on that one!\ntls: options: default: minVersion: VersionTLS13 Now, at first glance, this looks like this will solve all of your problems. But if you re-test at this point, you\u0026rsquo;ll notice that the \u0026lsquo;TLS\u0026rsquo; section of the report shows a yellow \u0026lsquo;No\u0026rsquo; by TLS 1.2. This is because TLS 1.2 isn\u0026rsquo;t deprecated yet, it\u0026rsquo;s still perfectly usable and certain individuals who might not have upgraded their browsers in quite some time (or Java 8\u0026hellip;) might not have full TLS 1.3 support.\nSurely that\u0026rsquo;s easy to fix, right? Just amend the \u0026lsquo;13\u0026rsquo; to a \u0026lsquo;12\u0026rsquo; so that your minimum version of TLS is 1.2 and clients can negotiate higher if they like.\ntls: options: default: minVersion: VersionTLS12 Now with this, you\u0026rsquo;ll still get an A grade, but the \u0026lsquo;Ciper Suites\u0026rsquo; section of the report will shed some more light here:\nNow, you don\u0026rsquo;t want to be seen by all of your webmaster friends to be supporting WEAK cipher suites, do you? That would just be embarrassing, every time you turn around and see 2 people whispering and pointing in your direction, you\u0026rsquo;d know exactly what they were talking about! Let\u0026rsquo;s solve that before it becomes a problem. Traefik has a \u0026lsquo;cipherSuites\u0026rsquo; key that we can add to the default configuration which means that all of our sites will automatically only accept these and best of all, we only have to define it once! I\u0026rsquo;ve defined a few accepted ones here, but please DYOR (Do Your Own Research) to make sure that you\u0026rsquo;re happy to support these, or possibly to add more. Our final file, therefore, becomes the below:\ntls: options: default: minVersion: VersionTLS12 cipherSuites: - TLS_AES_128_GCM_SHA256 - TLS_CHACHA20_POLY1305_SHA256 - TLS_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 And you\u0026rsquo;ll also notice the report now shows:\nCongrats, you\u0026rsquo;ve made the grade! :confetti_ball:\nIn a future post, we\u0026rsquo;ll go over upgrading this grade to an A+!\nFull dynamic.yml File tls: options: default: minVersion: VersionTLS12 cipherSuites: - TLS_AES_128_GCM_SHA256 - TLS_CHACHA20_POLY1305_SHA256 - TLS_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 Full traefik.yml (Static) File entryPoints: http: address: \u0026#34;:80\u0026#34; https: address: \u0026#34;:443\u0026#34; providers: docker: {} file: filename: /etc/traefik/dynamic.yml certificatesResolvers: le: acme: email: name@domain.tld storage: /letsencrypt/acme.json tlsChallenge: {} Full docker-compose.yml File version: \u0026#34;3.3\u0026#34; services: traefik: container_name: traefik image: traefik:v2.4.8 labels: - \u0026#34;traefik.enable=true\u0026#34; ### Middleware Redirect - \u0026#34;traefik.http.middlewares.https-redirect.redirectscheme.scheme=https\u0026#34; ### Global HTTP -\u0026gt; HTTPS Redirect - \u0026#34;traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\u0026#34; - \u0026#34;traefik.http.routers.redirs.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.redirs.middlewares=https-redirect\u0026#34; - \u0026#34;traefik.http.routers.redirs.priority=1\u0026#34; ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; - \u0026#34;certs:/letsencrypt\u0026#34; - \u0026#34;./traefik.yml:/etc/traefik/traefik.yml\u0026#34; - \u0026#34;./dynamic.yml:/etc/traefik/dynamic.yml\u0026#34; restart: unless-stopped networks: - proxy ","permalink":"https://dangerous.tech/achieving-an-a-grade-with-traefik/","summary":"\u003cp\u003eIn my \u003ca href=\"https://dangerous.tech/learning-to-love-yaml-converting-your-traefik-setup-from-cmdline-into-yaml/\"\u003eprevious post\u003c/a\u003e, I detailed moving your Traefik configuration to a slightly more permanent format (YAML\u0026hellip; I know, just go with it). You might have then been tempted to go to SSL Labs and do an \u003ca href=\"https://www.ssllabs.com/ssltest/\"\u003eSSL Test\u003c/a\u003e. What you would have discovered, in that tragic scenario, is a mere B grade!\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/20210524/pre-config.png\" alt=\"Pre-Config\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;Why, oh why?\u0026rdquo; I hear you cry, \u0026ldquo;I did everything right!\u0026rdquo; Well, technically you did, so don\u0026rsquo;t feel too bad. It\u0026rsquo;s not a fault of yours that you\u0026rsquo;re letting people with filthy TLS 1.0 and 1.1 view your site. It is, however, the view of \u003ca href=\"https://datatracker.ietf.org/doc/rfc8996/\"\u003ecertain industry bodies\u003c/a\u003e that these technologies are deprecated and therefore have multiple nasty security vulnerabilities.\u003c/p\u003e","title":"Achieving an A Grade with Traefik"},{"content":"In this post, I shared my Traefik configuration. Back then, I\u0026rsquo;d just learned about Traefik and followed a few posts to set things up and get them working with all of my containers. Since then, I\u0026rsquo;ve used Traefik for every new container I\u0026rsquo;ve set up, I\u0026rsquo;ve run servers like Minecraft and 7 Days to Die through it, as well as numerous different DevOps-adjacent containers. So whilst I\u0026rsquo;m no means an expert, I\u0026rsquo;m definitely a little more adept at some of its configuration nuances (I\u0026rsquo;ll be publishing something next week about how to globally disable TLS 1.0 and 1.1 so keep an eye out for that).\nCaveats As per normal, ensure the below are met before starting this:\nYou\u0026rsquo;ve backed up your current config, certs volume, docker-compose.yml file and/or anything related to the running version of Traefik You\u0026rsquo;re doing this with Traefik in a Docker container The same principal applies to a natively running version of Traefik I suppose, just ignore the bit about passing the file through to the container You understand basic Docker concepts such as bringing a container up and down, and managing your configuration through a docker-compose.yml file Static or Dynamic? The best descriptor of the differences is probably the image above. All you really need to be concerned about is the following:\nStatic runs at startup Dynamic runs each time a new request comes in to Traefik Because of this distinction, there are certain elements that fit naturally into each of these categories. In the current config (located at the bottom of the post), Static options are provided via the \u0026lsquo;command\u0026rsquo; key. There are certain Dynamic options that can be specified in the \u0026rsquo;labels\u0026rsquo; key (the normal router creation, certresolver binding, etc) but we won\u0026rsquo;t go over those in detail just yet.\nConverting YAML Into\u0026hellip; YAML? Technically, to convert from the cmdline style flags to a permanent YAML file we\u0026rsquo;re going from docker-compose.yml to traefik.yml, but you\u0026rsquo;ll see that the format will wind up looking a little different. If you\u0026rsquo;ve worked with JSON at all, the way to split these up should look rather familiar. The basic gist is that everywhere you see a period, you insert a new line.\nAs an example, the command argument\n- --entrypoints.http.address=:80\nwill translate into\nentryPoints: http: address: 80 Create a file called traefik.yml in the same directory as your docker-compose.yml file (or don\u0026rsquo;t, I\u0026rsquo;m not your dad\u0026hellip; Just don\u0026rsquo;t forget to amend the final volume mapping below) and convert these lines into sweet, sweet YAML. Once you\u0026rsquo;ve completed the whole \u0026lsquo;command\u0026rsquo; key, you should have a file like the below (certain config will accept either true or {} as valid to turn them on, always worth double checking the docs for which is preferred):\nentryPoints: http: address: \u0026#34;:80\u0026#34; https: address: \u0026#34;:443\u0026#34; providers: docker: {} certificatesResolvers: le: acme: email: name@email.tld storage: /letsencrypt/acme.json tlsChallenge: {} That Rug Really Tied The Room Together Now you have a static file on the host system, viola. Remove the \u0026lsquo;command\u0026rsquo; key, rebuild the container and you\u0026rsquo;re ready to go!\nWhat\u0026rsquo;s that? Your container is giving you an error that it can\u0026rsquo;t find the file you just created? Ahhh\u0026hellip; We probably should have actually utilised that new file somehow, huh?\nWell then, ignoring that small blunder, just add a line to your volume mappings to make the file visible inside the container (/etc/traefik/traefik is the default location that Traefik will look for config). This is where you change the left hand side of the mapping if you located your traefik.yml file elsewhere\u0026hellip;\n- \u0026quot;./traefik.yml:/etc/traefik/traefik.yml\u0026quot;\nand you\u0026rsquo;re off to the races! Just bring the container down (you are storing your certs in a separate volume, riiiiiiight?) and back up again and Traefik will now reload, giving you this nifty little confirmation:\nNow, if you ever want to add configuration to the Static options for Traefik, you just have to add them to this file and restart the container. The documentation is always a little bit more helpful when using YAML, as all things aren\u0026rsquo;t necessarily formatted for cmdline on the website.\nPrevious Traefik Config version: \u0026#34;3.3\u0026#34; services: traefik: container_name: traefik image: traefik:v2.4.8 command: # - --log.level=DEBUG # Entrypoints - --entrypoints.http.address=:80 - --entrypoints.https.address=:443 # Provider Info - --providers.docker # Certificate Resolver Info - --certificatesresolvers.le.acme.email=name@email.tld - --certificatesresolvers.le.acme.storage=/letsencrypt/acme.json - --certificatesresolvers.le.acme.tlschallenge=true labels: - \u0026#34;traefik.enable=true\u0026#34; ### Middleware Redirect - \u0026#34;traefik.http.middlewares.https-redirect.redirectscheme.scheme=https\u0026#34; ### Global HTTP -\u0026gt; HTTPS Redirect - \u0026#34;traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\u0026#34; - \u0026#34;traefik.http.routers.redirs.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.redirs.middlewares=https-redirect\u0026#34; - \u0026#34;traefik.http.routers.redirs.priority=1\u0026#34; ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; - \u0026#34;certs:/letsencrypt\u0026#34; restart: unless-stopped networks: - proxy volumes: certs: networks: proxy: driver: bridge New Traefik Config version: \u0026#34;3.3\u0026#34; services: traefik: container_name: traefik image: traefik:v2.4.8 labels: - \u0026#34;traefik.enable=true\u0026#34; ### Middleware Redirect - \u0026#34;traefik.http.middlewares.https-redirect.redirectscheme.scheme=https\u0026#34; ### Global HTTP -\u0026gt; HTTPS Redirect - \u0026#34;traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\u0026#34; - \u0026#34;traefik.http.routers.redirs.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.redirs.middlewares=https-redirect\u0026#34; - \u0026#34;traefik.http.routers.redirs.priority=1\u0026#34; ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; - \u0026#34;certs:/letsencrypt\u0026#34; - \u0026#34;./traefik.yml:/etc/traefik/traefik.yml\u0026#34; restart: unless-stopped networks: - proxy volumes: certs: networks: proxy: driver: bridge ","permalink":"https://dangerous.tech/learning-to-love-yaml-converting-your-traefik-setup-from-cmdline-into-yaml/","summary":"\u003cp\u003eIn \u003ca href=\"https://dangerous.tech/traefik-on-docker-a-modern-reverse-proxy-setup/\"\u003ethis post\u003c/a\u003e, I shared my Traefik configuration. Back then, I\u0026rsquo;d just learned about Traefik and followed a few posts to set things up and get them working with all of my containers. Since then, I\u0026rsquo;ve used Traefik for every new container I\u0026rsquo;ve set up, I\u0026rsquo;ve run servers like Minecraft and 7 Days to Die through it, as well as numerous different DevOps-adjacent containers. So whilst I\u0026rsquo;m no means an expert, I\u0026rsquo;m definitely a little more adept at some of its configuration nuances (I\u0026rsquo;ll be publishing something next week about how to globally disable TLS 1.0 and 1.1 so keep an eye out for that).\u003c/p\u003e","title":"Learning to Love YAML - Converting Your Traefik Setup From Cmdline into YAML"},{"content":"The previous post on this topic is still relevant, however, since my selection of a new reverse proxy, I felt it necessary to publish an update.\nAssumptions You know what HTTPS and SSL certificates are/do and how the CA ecosystem works You have a domain that you plan to use for the Jenkins instance and this domain either doesn\u0026rsquo;t have a CAA record or has one that allows LetsEncrypt You understand the general working of containers and have docker installed on your system. Docker/Docker Compose knowledge is a plus but not required, you can basically just copy/paste code to get this working. I\u0026rsquo;ll also go over some common pitfalls at the end You either have your own reverse proxy setup and can handle this or you\u0026rsquo;re using my Traefik config You know what Jenkins is and why you should be using it. If you don\u0026rsquo;t, use the Google machine OK, let\u0026rsquo;s get this show on the road.\nPrerequisites A linux environment - because there\u0026rsquo;s a special circle of hell reserved for you if you\u0026rsquo;re doing this on Windows (just kidding\u0026hellip;) Docker Docker-compose A domain and the ability to add A records Tea/Coffee because caffeine 1 - Checking Your Install At the time of writing, I\u0026rsquo;ve got the docker and docker-compose versions below. If these become wildly out of date I\u0026rsquo;ll try to update in the future. Or shoot me an angry email on the About Me page and I\u0026rsquo;ll get right to it! If you can\u0026rsquo;t be bothered to read the walkthrough and just want the files, they\u0026rsquo;re pasted at the bottom of the post. I gotchu.\nAs long as your docker --version and docker-compose --version outputs are these versions or later, this should work fine. If they\u0026rsquo;re not, check if you\u0026rsquo;ve got installed via snap or pip or something as they sometimes don\u0026rsquo;t update versions frequently. Official docker install process is here if you need it\nYou can also run docker run hello-world to make sure containers are running properly.\n2 - Adding Your A Record Before we get all the nifty LetsEncrypt bits done, you need to have a domain that resolved to the IP of your server/raspberry pi/nan\u0026rsquo;s old laptop. Hopefully you\u0026rsquo;ve got a VPS with a public IPv4 adress you can use, if not get comfortable with port forwarding. You\u0026rsquo;ll need to forward TCP 80 and 443 inbound because LetsEncrypt still uses 80 for one of the validation challenges.\nCreate an A record however the Good Lord GoDaddy lets you (or use a respectable registrar/DNS provider ;)). This can be on the root domain or any subdomain, we\u0026rsquo;re not getting fancy and doing wildcard certificates here, I\u0026rsquo;ll probably cover that in another post.\n3 - Compose Like Vivaldi OK so here\u0026rsquo;s where we get into the docker-compose magic. There\u0026rsquo;s another post explaining the proxy in detail so if you\u0026rsquo;d like to read up on that, check it out here. You can grab the correct proxy config from that of this page and just docker-compose up -d that bad boy. Therefore, we\u0026rsquo;ll jump straight to the Jenkins YAML file.\nversion: \u0026#34;3\u0026#34; services: jenkins: container_name: jenkins image: jenkins/jenkins:lts restart: unless-stopped We start with the basics, version and services because we\u0026rsquo;re keeping to the standard like good, rule followersâ„¢. Next, we name the service and container something sensible and pull the jenkins/jenkins:lts image. You can use the :latest tag instead of :lts if you want the bleeding edge releases but I\u0026rsquo;d like my build server to be nice stable personally\u0026hellip; restart: unless-stopped has the advantage of restarting the container automatically if you reboot the OS of the machine that it\u0026rsquo;s running on but doesn\u0026rsquo;t get too aggressive if there\u0026rsquo;s a critical error that you need to stop the container for.\nlabels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jenkins.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.jenkins.rule=Host(`sub.domain.tld`)\u0026#34; - \u0026#34;traefik.http.routers.jenkins.tls.certresolver=le\u0026#34; - \u0026#34;traefik.http.services.jenkins.loadbalancer.server.port=8080\u0026#34; Labels are how we tell Traefik how to route traffic for this container. Because Jenkins for some reason won\u0026rsquo;t let you change the port that it\u0026rsquo;s running on, you always have to proxy across to 8080. The loadbalancer label means that the proxy will take 443 traffic (designated by the entrypoint=https (more on this in the post about my Traefik setup)) on whatever the Host URL is and forward it to 8080 on the container so Jenkins is none the wiser. Neat!\nvolumes: - jenkins_home:/var/jenkins_home networks: - traefik_proxy volumes: jenkins_home: networks: traefik_proxy: external: true Next there\u0026rsquo;s the volume goodness. Fairly standard, nothing special here. If you want to get creative and map a specific local place in the filesystem you can change the named mapping to a bind mount, as detailed in the docker docs. We have to declare this named volume at the bottom by itself because we\u0026rsquo;re being rule followersâ„¢, remember?\n4 - Run Forrest, Run Now, it\u0026rsquo;s time to put the butler to work. Worth mentioning that you should have your proxy up and running by this point, docker-compose up -d if you dont. docker-compose up -d \u0026amp;\u0026amp; docker-compose logs -f will do the trick. It\u0026rsquo;ll grab the relevant images, start like magic on the first attempt and then show you the logs of a fledgling jetty server doing it\u0026rsquo;s thing. You\u0026rsquo;ll also, crucially, see the below little chunk of text.\nGrab that password and browse to the domain that you\u0026rsquo;ve pointed at this server. If you get some weird browser error, give it 30 seconds, dump the cache and refresh. The proxy container probably just took a second to get all the certs in order. If all went well, you should be greeted with the below screen.\nYou guessed it, stick that admin password in and hit Continue!\nThis one depends on if you know what you\u0026rsquo;re doing, I normally just hit the \u0026lsquo;Install suggested plugins\u0026rsquo; options. You can always pick other specific ones later on.\nThen some nifty plugin installing will commence. This will take a minute so grab a cup of tea. Containerising without tea is uncivilised, you should know that already!\nNow you\u0026rsquo;ll create an admin user. Bob the builder is really the only user that makes sense in the context of creating a build server but if you have some other cool name like Jeffrey or Federcio then go ahead and enter that instead. I can\u0026rsquo;t stop you from making the wrong decisions, I can only try and advise you against them\u0026hellip; Also we know Bob The Builder is the type of dude to still have an AOL email account, don\u0026rsquo;t @ me.\nThis next page should have picked up your install URL because you\u0026rsquo;ve browsed to it but if not, pop the FQDN in here to avoid any wrongly genearted links in the future.\nCongratuwelldone! The butler is at your service.\nCan you smell that? That new server smell? Smells like hope with notes of long nights figuring out why it builds locally but Jenkins hates you. That\u0026rsquo;s all yet to come, friends. I\u0026rsquo;ll be writing more Jenkins posts as I set up build pipelines in various differnet environments so keep your eyes peeled for those scintillating pieces of writing.\n","permalink":"https://dangerous.tech/update-jenkins-with-https-in-a-docker-container/","summary":"\u003cp\u003e\u003cem\u003eThe \u003ca href=\"https://dangerous.tech/jenkins-with-https-in-a-docker-container/\"\u003eprevious post\u003c/a\u003e on this topic is still relevant, however, since my selection of a \u003ca href=\"https://dangerous.tech/selecting-a-new-reverse-proxy-caddy-or-traefik/\"\u003enew reverse proxy\u003c/a\u003e, I felt it necessary to publish an update.\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"assumptions\"\u003eAssumptions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eYou know what HTTPS and SSL certificates are/do and how the CA ecosystem works\u003c/li\u003e\n\u003cli\u003eYou have a domain that you plan to use for the Jenkins instance and this domain either doesn\u0026rsquo;t have a CAA record or has one that allows LetsEncrypt\u003c/li\u003e\n\u003cli\u003eYou understand the general working of containers and have docker installed on your system. \u003cem\u003eDocker/Docker Compose knowledge is a plus but not required, you can basically just copy/paste code to get this working. I\u0026rsquo;ll also go over some common pitfalls at the end\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eYou either have your own reverse proxy setup and can handle this or you\u0026rsquo;re using \u003ca href=\"https://dangerous.tech/traefik-on-docker-a-modern-reverse-proxy-setup/\"\u003emy Traefik config\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eYou know what Jenkins is and why you should be using it. If you don\u0026rsquo;t, use the \u003ca href=\"https://lmgtfy.com/?q=why+is+jenkins+awesome\"\u003eGoogle machine\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOK, let\u0026rsquo;s get this show on the road.\u003c/p\u003e","title":"UPDATE: Jenkins with HTTPS in a Docker Container"},{"content":"Twitter? Garbage. Facebook? Garbage. Do you enjoy having your data harvested and sold? No? Good. Back in the day, there was this thing called Internet Relay Chat. What if I told you\u0026hellip; It still exists! Internet socializing without all the Silicon Valley data theft! But due to how the ecosystem works, any time you\u0026rsquo;re not actively connected to the server, you don\u0026rsquo;t receive any messages. This means that you don\u0026rsquo;t get to see conversations that happen while you\u0026rsquo;re offline, even if somebody mentions you!\nEnter the IRC Bouncer. This nifty little piece of software sits behind the scenes and stays connected to the IRC server(s) whilst you\u0026rsquo;re sleeping, working or doing other things that happen IRL. Then when you return to where you\u0026rsquo;re supposed to be (i.e. in front of a computer) you reconnect and the bouncer greets you with a flood of glorious, nerdy conversations.\nAssumptions You know why HTTPS is necessary and will use it You understand the general working of containers and have docker installed on your system. (Docker/Docker Compose knowledge is a plus but not required, you can basically just copy/paste code to get this working.) Your domain/subdomain has a CAA record that supports LetsEncrypt You Will Need A linux environment - or WSL I guess, if you hate yourself\u0026hellip; which is actually pretty good now Docker Docker-compose An A record pointed at your environment\u0026rsquo;s public IP (for LetsEncrypt to validate the SSL cert) Your caffeinated beverage of choice 1 - Checking Your Install See Step 1 from this post for further version instructions. The short of it is that I\u0026rsquo;m using docker 19.03.1 and docker-compose 1.21.0.\n2 - Generate a Basic ZNC Config As per usual, we\u0026rsquo;re using a reverse proxy for this component so if you\u0026rsquo;re not using that, you can either ignore those particular parts or you can set that sucker up using the instructions in this post. The config is also on GitHub.\nBefore we compose all the things, we first have to generate the base ZNC config. If you\u0026rsquo;re migrating from an old install, just migrate the whole znc folder to /var/lib/docker/volumes/YOURVOLUMENAME/_data/.\n2.1 - Generate Config Run docker run -it -v znc_znc_data:/znc-data znc --makeconf then follow the process through like the below (you should customize your username, nicks, ident and bind host though).\n[ .. ] Checking for list of available modules... [ ** ] [ ** ] -- Global settings -- [ ** ] [ ?? ] Listen on port (1025 to 65534): 6697 [ ?? ] Listen using SSL (yes/no) [no]: yes [ .. ] Verifying the listener... [ ** ] Unable to locate pem file: [/znc-data/znc.pem], creating it [ .. ] Writing Pem file [/znc-data/znc.pem]... [ ** ] Enabled global modules [webadmin] [ ** ] [ ** ] -- Admin user settings -- [ ** ] [ ?? ] Username (alphanumeric): yournamehere [ ?? ] Enter password: nicensecure [ ?? ] Confirm password: nicensecure [ ?? ] Nick [yournamehere]: yournickhere [ ?? ] Alternate nick [yournamehere_]: youraltnickhere [ ?? ] Ident [yournamehere]: youridenthere [ ?? ] Real name (optional): yourmeatspacenamehere [ ?? ] Bind host (optional): leavethisblankit\\\u0026#39;spointless [ ** ] Enabled user modules [chansaver, controlpanel] [ ** ] [ ?? ] Set up a network? (yes/no) [yes]: no [ ** ] [ .. ] Writing config [/znc-data/configs/znc.conf]... [ ** ] [ ** ] To connect to this ZNC you need to connect to it as your IRC server [ ** ] using the port that you supplied. You have to supply your login info [ ** ] as the IRC server password like this: user/network:pass. [ ** ] [ ** ] Try something like this in your IRC client... [ ** ] /server \u0026lt;znc_server_ip\u0026gt; +6697 yournamehere:\u0026lt;pass\u0026gt; [ ** ] [ ** ] To manage settings, users and networks, point your web browser to [ ** ] https://\u0026lt;znc_server_ip\u0026gt;:6697/ [ ** ] [ ?? ] Launch ZNC now? (yes/no) [yes]: no 2.2 - Add a Listener to ZNC You can do this in 1 of 2 ways:\nLog into the webadmin GUI, go to the Global Settings section and add the listener there.\nEdit the config directly with vim /var/lib/docker/volumes/YOURVOLUMENAME/_data/configs/znc.conf\n\u0026lt;Listener listener0\u0026gt; AllowIRC = true AllowWeb = false IPv4 = true IPv6 = false Port = 6697 SSL = true URIPrefix = / \u0026lt;/Listener\u0026gt; \u0026lt;Listener listener1\u0026gt; AllowIRC = false AllowWeb = true IPv4 = true IPv6 = false Port = 8080 SSL = false URIPrefix = /znc/ \u0026lt;/Listener\u0026gt; The listeners basically operate thusly: one is for the actual IRC network connections and one is for the webadmin frontend. We only need to reverse proxy the web GUI port, and that\u0026rsquo;s the only easy one to do with Traefik. The ZNC Wiki has some opinions on using NGINX for that purpose but I haven\u0026rsquo;t tried it so your mileage may vary. I don\u0026rsquo;t really see the extra benefits of reverse proxying the IRC connection but maybe I will have my eyes opened in the future, at which point I\u0026rsquo;ll update this post. Also, you can use whatever port you like for the web listener, we\u0026rsquo;re going to forward that from 443 -\u0026gt; containerPort by Traefik so as long as you\u0026rsquo;re consistent then you\u0026rsquo;re alright.\n2.3 Make a Compose File and Add Traefik Container Labels Because we\u0026rsquo;re not animals who just run containers with a command and then leave, we need to pop this into a docker-compose.yml file. As per the general Traefik config, we have to add the labels to our ZNC container so that our traffic gets routed properly.\nversion: \u0026#34;3\u0026#34; services: znc: container_name: znc image: znc:1.8.0 restart: unless-stopped ports: - 6697:6697 labels: - \u0026#34;traefik.enable=true\u0026#34; # Web Frontend Rules - \u0026#34;traefik.http.routers.znc.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.znc.rule=Host(`sub.domain.tld`)\u0026#34; - \u0026#34;traefik.http.routers.znc.tls.certresolver=le\u0026#34; - \u0026#34;traefik.http.services.znc.loadbalancer.server.port=8080\u0026#34; volumes: - znc_data:/znc-data networks: - traefik_proxy volumes: znc_data: networks: traefik_proxy: external: true This one\u0026rsquo;s pretty simple, all in all. Because we\u0026rsquo;re not reverse proxying the IRC connection, this is just a case of adding your domain to the Host rule so that the container gets an SSL cert and using the correct port in the loadbalancer section (make it the same as the one from the listener above and you\u0026rsquo;ll be fine). Oh and make sure to reference your Traefik proxy network as external (you definitely created one of those, right?).\n3 - Extract The Certs From Traefik The above will get you up and running with a cert, but at the moment the ZNC config is simply pointing to the znc.pem file that it created way back in step 2.1. If you look in your znc.conf file, you\u0026rsquo;ll see these sections right at the top (the cert names may be different, I\u0026rsquo;m not creating this from scratch just to get that detail right\u0026hellip; the options are what\u0026rsquo;s important):\nSSLCertFile = /znc-data/certificate.pem SSLDHParamFile = /znc-data/dhparam.pem SSLKeyFile = /znc-data/privatekey.pem You\u0026rsquo;ll notice, if you go and have a look at your Traefik data volume, that all you have in there is an acme.json file. This file is where Traefik keeps all your certs, but ZNC won\u0026rsquo;t read such a file as it\u0026rsquo;s looking for .pem files.\nEnter traefik-certs-dumper (referred to henceforth as TCD for brevity). This is a really neat container that you can just add to your Traefik docker-compose.yml file in order to export the certificates as their own separate files. This will mean all we have to do is copy 2 of those and ZNC will work nicely.\ntraefik-certs-dumper: image: ldez/traefik-certs-dumper:v2.7.0 entrypoint: sh -c \u0026#39;traefik-certs-dumper file --version v2 --domain-subdir --crt-ext=.pem --key-ext=.pem --watch --source /data/acme.json --dest /data/certs/\u0026#39; labels: - \u0026#34;traefik.enable=false\u0026#34; volumes: - \u0026#34;certs:/data\u0026#34; As per usual, specify a version. If this gets a breaking change, it has the potential to affect Traefik, which will affect every container that you\u0026rsquo;re reverse proxying, so be careful when upgrading. Read the changelog. One key here is that the volume that TCD uses must be the same as Traefik. This is because they need to access the same data (that acme.json file), so there\u0026rsquo;s no point creating a new volume and copying it across.\nI\u0026rsquo;ve tweaked the entrypoint command here from a few tries at this with the various options from the README file in the repo.\n--version v2 tells TCD that we\u0026rsquo;re using Traefik v2.\n--domain-subdir tells TCD to put the certs in differnt directories based on the host subdomain. This will make the right files easy to find and also easier to script when I eventually get around to doing that.\n--cert-ext=.pem and --key-ext=.pem just gets us the files in the format that we want, saves conversion after the fact.\n--watch tells TCD to watch the JSON file for new certs and for cert updates which saves us the trouble of restarting the container ever.\nObviously --source and --dest tell TCD where to look for the JSON file and where to dump the cert files. The repo README seemed to prefer the /data location so that\u0026rsquo;s just where I mapped my volume and therefore, where the certs will go. You could always map a separate volume for the --dest if you liked.\n3.1 Give ZNC The Certs Now, once you add this to your docker-compose.yml and bring it up, you\u0026rsquo;ll notice a bunch of subdirectories in your --dest directory. The below is truncated for brevity.\n. â”œâ”€â”€ [-rw------- 117K] acme.json â””â”€â”€ [drwxr-xr-x 4.0K] certs â”œâ”€â”€ [drwxr-xr-x 4.0K] sub.domain.1 â”‚Â â”œâ”€â”€ [-rw-r--r-- 3.8K] certificate.pem â”‚Â â””â”€â”€ [-rw------- 3.2K] privatekey.pem â”œâ”€â”€ [drwxr-xr-x 4.0K] sub.domain.2 â”‚Â â”œâ”€â”€ [-rw-r--r-- 3.8K] certificate.pem â”‚Â â””â”€â”€ [-rw------- 3.2K] privatekey.pem â”œâ”€â”€ [drwxr-xr-x 4.0K] sub.domain.2 â”‚Â â”œâ”€â”€ [-rw-r--r-- 3.8K] certificate.pem â”‚Â â””â”€â”€ [-rw------- 3.2K] privatekey.pem â”œâ”€â”€ [drwxr-xr-x 4.0K] private â”‚Â â””â”€â”€ [-rw------- 3.2K] letsencrypt.pem All you have to do is copy the relevant .pem files over to the root of the ZNC volume. Make sure that the filename of the certs matches the entry in your znc.conf and you should be good to go. At some point in the future I\u0026rsquo;ll be taking advantage of the --post-hook option in order to have this magically copy whenever our cert is renewed but that\u0026rsquo;s an issue for another post.\nSmall warning here, leave the private folder alone, it\u0026rsquo;s what Traefik uses to authenticate you to LetsEncrypt. You don\u0026rsquo;t really have a need to use that so ignore it, \u0026lsquo;kay?\n4 - This Ain\u0026rsquo;t Your Grandaddy\u0026rsquo;s Chat System Now we need to bring it all together. As per the normal process, run docker-compose up -d for the magic to get underway.\nIf you\u0026rsquo;re monitoring the Traefik logs, you\u0026rsquo;ll see it pick up the new container event and obtain the SSL certificates. If you\u0026rsquo;re not, give it 10 seconds or so and then open a web browser to the domain you\u0026rsquo;ve used.\nNow sign in, add a network and get to connecting! Any further details can be found on the ZNC wiki which is pretty comprehensive.\nMy full ZNC docker-compose.yml file:\nversion: \u0026#34;3\u0026#34; services: znc: container_name: znc image: znc:1.8.0 restart: unless-stopped ports: - 6697:6697 labels: - \u0026#34;traefik.enable=true\u0026#34; # Web Frontend Rules - \u0026#34;traefik.http.routers.znc.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.znc.rule=Host(`sub.domain.tld`)\u0026#34; - \u0026#34;traefik.http.routers.znc.tls.certresolver=le\u0026#34; - \u0026#34;traefik.http.services.znc.loadbalancer.server.port=8080\u0026#34; volumes: - znc_data:/znc-data networks: - traefik_proxy volumes: znc_data: networks: traefik_proxy: external: true My full Traefik + TCD docker-compose.yml file:\nversion: \u0026#34;3.3\u0026#34; services: traefik: container_name: traefik image: traefik:v2.2.0 command: #- --log.level=DEBUG # Entrypoints - --entrypoints.http.address=:80 - --entrypoints.https.address=:443 # Provider Info - --providers.docker # Certificate Resolver Info - --certificatesresolvers.le.acme.email=your@domain.tld - --certificatesresolvers.le.acme.storage=/letsencrypt/acme.json - --certificatesresolvers.le.acme.tlschallenge=true labels: # Middleware Redirect - \u0026#34;traefik.http.middlewares.https-redirect.redirectscheme.scheme=https\u0026#34; # Global HTTP -\u0026gt; HTTPS Redirect - \u0026#34;traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\u0026#34; - \u0026#34;traefik.http.routers.redirs.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.redirs.middlewares=https-redirect\u0026#34; ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; - \u0026#34;certs:/letsencrypt\u0026#34; restart: unless-stopped networks: - proxy traefik-certs-dumper: image: ldez/traefik-certs-dumper:v2.7.0 entrypoint: sh -c \u0026#39;traefik-certs-dumper file --version v2 --domain-subdir --crt-ext=.pem --key-ext=.pem --watch --source /data/acme.json --dest /data/certs/\u0026#39; labels: - \u0026#34;traefik.enable=false\u0026#34; volumes: - \u0026#34;certs:/data\u0026#34; volumes: certs: networks: proxy: driver: bridge ","permalink":"https://dangerous.tech/znc--docker-a-containerized-irc-bouncer/","summary":"\u003cp\u003eTwitter? Garbage. Facebook? Garbage. Do you enjoy having your data harvested and sold? No? Good. Back in the day, there was this thing called Internet Relay Chat. What if I told you\u0026hellip; It still exists! Internet socializing without all the Silicon Valley data theft! But due to how the ecosystem works, any time you\u0026rsquo;re not actively connected to the server, you don\u0026rsquo;t receive any messages. This means that you don\u0026rsquo;t get to see conversations that happen while you\u0026rsquo;re offline, even if somebody mentions you!\u003c/p\u003e","title":"ZNC + Docker - A Containerized IRC Bouncer"},{"content":"For many years, I used NGINX as a reverse proxy. It\u0026rsquo;s pretty bulletproof as long as you have your configuration set up properly from the get-go. There\u0026rsquo;s even a number of ways to automatially create new SSL certs for new containers but this mostly has to be done in a separate container itself. This makes the process of upgrading, testing, rolling back, patching and whatnot a bit of a mither. Enter Traefik.\nIf you saw the comparison post on why I chose Traefik vs Caddy then you\u0026rsquo;ll know I liked both of them, but I\u0026rsquo;m glad that I made the choice that I did. Since that post, Traefik has gone fully 2.0, updated a lot of their documentaion and just keeps getting better and better. I will still keep a cursory eye on Caddy but I doubt I\u0026rsquo;ll be switching over any time soon.\nTraefik support the defining of configuration via command line arguments or via a TOML file. To keep things simple and not require the backing up of an extra file, I run my container with command line arguments. This means all of the config is stored within the docker-compose file and makes it rather easy to share. The full config will be at the end of the post, but for now, let\u0026rsquo;s go through it in smaller chunks.\nTraefik Container Walkthrough version: \u0026#34;3.3\u0026#34; services: traefik: container_name: traefik image: traefik:v2.2.0 command: Obviously this is just the headings and whatnot, what you name the service/container is up to you. The key bit is that you should peg your container to an actual version, not just latest, to save upgrading your container in the future and encountering breaking changes which destroys your reverse proxy. You will get log messages from time to time about upgrades if they are available. The command section is where we pass the command line arguments to the Traefik binary that will run in the container.\n#- --log.level=DEBUG Useful if you need to debug/troubleshoot your config (obviously) but it does produce a LOT of noise so disable once you\u0026rsquo;re up and running.\n# Entrypoints - --entrypoints.http.address=:80 - --entrypoints.https.address=:443 Entrypoints are essentially where you\u0026rsquo;re expecting traffic to come in to Traefik. Here, we set the address of an entrypoint called http and https to their relevant ports. You could very well define - --entrypoints.irc.address=:6697 if you liked.\n# Provider Info - --providers.docker Again, self-explannatory really. This tells Traefik that we\u0026rsquo;re dealing with docker containers. There are plenty of other supported providers out there too.\n# Certificate Resolver Info - --certificatesresolvers.le.acme.email=your@email.domain - --certificatesresolvers.le.acme.storage=/letsencrypt/acme.json - --certificatesresolvers.le.acme.tlschallenge=true This is probably my favourite part, the auto SSL cert retrieval. The le portion is configurable and this is what you\u0026rsquo;ll reference in your container setups, along with a Host(`sub.domain.tld`) rule. See the bottom of this post for an example using my Jenkins container.\nThat\u0026rsquo;s it for the basics, really. But, because I\u0026rsquo;m usually not content with the basics, here\u0026rsquo;s some more advanced config.\nlabels: # Middleware Redirect - \u0026#34;traefik.http.middlewares.https-redirect.redirectscheme.scheme=https\u0026#34; # Global HTTP -\u0026gt; HTTPS Redirect - \u0026#34;traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\u0026#34; - \u0026#34;traefik.http.routers.redirs.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.redirs.middlewares=https-redirect\u0026#34; This section creates a middleware element and a router to redirect all HTTP traffic to HTTPS. Setting this as a label on the Traefik container itself means that you don\u0026rsquo;t have to create individual middleware for each new container and then add the redir config into those containers routers in order for this to function. Much simpler and I haven\u0026rsquo;t found a problem with it thus far (I\u0026rsquo;m running 8-10 containers at any one time).\nports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; - \u0026#34;certs:/letsencrypt\u0026#34; restart: unless-stopped networks: - proxy volumes: certs: networks: proxy: driver: bridge The end part is all fairly self-explannatory too, if you\u0026rsquo;ve worked with containers before. We set up a volume to hold our certs JSON file, and we create the proxy network, which on my host is the only network which communicates with the outside world. We need to bind port 80 and 443 to this container because they will always be handling the public traffic routing, that\u0026rsquo; kind of what a reverse proxy is for\u0026hellip;\nBy virtue of the --provider=docker flag we went through earlier, Traefik knows that it should be looking for containers on this network so that\u0026rsquo;s what it will check when it receives new events from the /var/run/docker.sock which we\u0026rsquo;ve allowed it to see. That means that you don\u0026rsquo;t have to expose port 80 of 443 in the individual containers\u0026rsquo; docker-compose.yml files, you just have to add a few labels to point Traefik in the right direction (and yes, it supports external -\u0026gt; internal port redirection too so those containers with fixed internal ports can be mapped just fine).\nClient Container Walkthrough We\u0026rsquo;ll leave out the boring headers this time, the same advice applies from the Traefik header portion.\nlabels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jenkins.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.jenkins.rule=Host(`sub.domain.tld`)\u0026#34; - \u0026#34;traefik.http.routers.jenkins.tls.certresolver=le\u0026#34; - \u0026#34;traefik.http.services.jenkins.loadbalancer.server.port=8080\u0026#34; The labels section is the key bit of client config we need for Traefik. This tells Traefik to enable this container for reverse proxy goodness, to use our entrypoint (https) and out certresolver (le) which we defined earlier, as well as uses the Host(`sub.domain.tld`) rule.\nThe new thing here is the loadbalancer rule, which is how you can enable the external -\u0026gt; internal port mapping that I mentioned earlier. This will cause cause Traefik to route incoming traffic on the entrypoint (https/:443 here) to the specified internal port (8080 here) of the container.\nnetworks: - traefik_proxy networks: traefik_proxy: external: true Here we make sure that the container has the traefik_proxy network attached as an external network. This was defined in the Traefik container simply as proxy but Docker references these things like so: service_thing so if you run docker network list then you\u0026rsquo;ll see the correct network to add. You can also run docker network inspect your_proxy_network and look at the \u0026quot;Containers\u0026quot; key to see that the traefik container is in there.\nFull Traefik Container YAML version: \u0026#34;3.3\u0026#34; services: traefik: container_name: traefik image: traefik:v2.2.0 command: #- --log.level=DEBUG # Entrypoints - --entrypoints.http.address=:80 - --entrypoints.https.address=:443 # Provider Info - --providers.docker # Certificate Resolver Info - --certificatesresolvers.le.acme.email=your@email.domain - --certificatesresolvers.le.acme.storage=/letsencrypt/acme.json - --certificatesresolvers.le.acme.tlschallenge=true labels: # Middleware Redirect - \u0026#34;traefik.http.middlewares.https-redirect.redirectscheme.scheme=https\u0026#34; # Global HTTP -\u0026gt; HTTPS Redirect - \u0026#34;traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\u0026#34; - \u0026#34;traefik.http.routers.redirs.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.redirs.middlewares=https-redirect\u0026#34; ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; - \u0026#34;certs:/letsencrypt\u0026#34; restart: unless-stopped networks: - proxy volumes: certs: networks: proxy: driver: bridge Full Client (Jenkins) Container Walkthrough version: \u0026#39;3\u0026#39; services: jenkins: container_name: jenkins image: jenkins/jenkins:lts restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.http.routers.jenkins.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.jenkins.rule=Host(`sub.domain.tld`)\u0026#34; - \u0026#34;traefik.http.routers.jenkins.tls.certresolver=le\u0026#34; - \u0026#34;traefik.http.services.jenkins.loadbalancer.server.port=8080\u0026#34; volumes: - jenkins_home:/var/jenkins_home networks: - traefik_proxy volumes: jenkins_home: networks: traefik_proxy: external: true ","permalink":"https://dangerous.tech/set-up-traefik-on-docker-a-modern-reverse-proxy-solution/","summary":"\u003cp\u003eFor many years, I used \u003ca href=\"https://github.com/dangeroustech/docker-composes/tree/master/proxy\"\u003eNGINX as a reverse proxy.\u003c/a\u003e It\u0026rsquo;s pretty bulletproof as long as you have your configuration set up properly from the get-go. There\u0026rsquo;s even a number of ways to automatially create new SSL certs for new containers but this mostly has to be done in a separate container itself. This makes the process of upgrading, testing, rolling back, patching and whatnot a bit of a mither. Enter Traefik.\u003c/p\u003e","title":"Set Up Traefik on Docker - A Modern Reverse Proxy Solution"},{"content":"When first moving to i3 as a window manager, you may have relished the opportunity to learn a tonne of new shortcut keys on your journey to being the neckbeardiest of them all (I know I certainly did). At some point along the way, you probably realised that it would be useful to start certain programs/scripts when you first login. As this is linux, there\u0026rsquo;s about 15 different ways to do this, but I\u0026rsquo;m going to show you my favourite. It\u0026rsquo;s my favourite for a few reasons:\nYou can keep all of your startup scripts in one place (likely ~/.config/i3/scripts if you\u0026rsquo;re following my example) If you ever close one of these and want to start them again, just hit your i3 config reload key (mod + shift + r if you\u0026rsquo;re using my dotfiles) If you ever need to modify the way that the program is launched, you just have to tweak the script, no extra lines in your i3 config file Ingredients Your i3 config file (hopefully ~/.config/i3/config) The ability to write a shell script (may I recommend Neovim for even more neckbeard clout) Method 1 - Create a Shell Script So the way we\u0026rsquo;re going to do this is to run a shell script from within our i3 config, then the shell script is going to run the program. You could just add the program itself as a line in your i3 config file but there\u0026rsquo;s 2 major benefits to doing it this way:\nPID detection - we don\u0026rsquo;t want to run a new copy of the program every time we reload our i3 config, that will get messy fast Less clutter in the i3 config - the i3 config file is already a bit of chunky one, let\u0026rsquo;s not add to that, eh? Using your editor of choice create a new Shell script (I\u0026rsquo;m using this to launch Nextcloud, hence the below):\nif [[ ! $(pgrep -u $UID -x nextcloud) ]]; then nextcloud \u0026amp; fi All this is doing is using pgrep to check for a PID already launched by your User ID, to avoid double/triple/etc launching the same thing. If that\u0026rsquo;s not found, it will launch a new instance of that (and background it with \u0026amp; otherwise the script will hold up the loading of i3, which nobody wants). It\u0026rsquo;s worth mentioning that if the process has a different name than you\u0026rsquo;re expecting then this will fail. Best to run pgrep -u $UID -x PROCESS_NAME when it\u0026rsquo;s active just to be sure\n2 - Add a Reference to the i3 Config File Add the below to your i3 config file, assuming your script is in the same location as mentioned above (mine is below my Polybar config but you can just stick it at the end):\nexec_always --no-startup-id $HOME/.config/i3/scripts/YOUR_SCRIPT.sh Now reload i3 and viola, your program has launched. Now reload it again, maybe even a few times. You should notice that if it\u0026rsquo;s already launched, the i3 reload won\u0026rsquo;t relauch it, but if you close it the reload will launch it. Magic!\n","permalink":"https://dangerous.tech/running-a-script-at-login-with-i3/","summary":"\u003cp\u003eWhen first moving to i3 as a window manager, you may have relished the opportunity to learn a tonne of new shortcut keys on your journey to being the neckbeardiest of them all (I know I certainly did). At some point along the way, you probably realised that it would be useful to start certain programs/scripts when you first login. As this is linux, there\u0026rsquo;s about 15 different ways to do this, but I\u0026rsquo;m going to show you my favourite. It\u0026rsquo;s my favourite for a few reasons:\u003c/p\u003e","title":"Running a Script at Login with i3"},{"content":"Managing .gitconfig For Fun And Profit You know when you see that little green \u0026lsquo;Verified\u0026rsquo; badge next to your commits on GitHub? The one that makes it feel all official? Notice how when you commit and push from the command line you don\u0026rsquo;t get that by default? If you\u0026rsquo;ve ever wondered why, this post is for you! Also if you realise that signing your commits is a good idea generally because something something security\u0026hellip; It works for you too.\nPrerequisites gpg installed on the system you\u0026rsquo;re setting up commit signing from (gpg --version or gpg2 --version to verify) The ability to reach GitHub. This can be from the same machine, or you can be SSHed in, you\u0026rsquo;ll need to copy an exported key from the commandline to a browser window and you don\u0026rsquo;t want to type it all out\u0026hellip; 1 - Generate the Key Run gpg --full-generate-key to kick off the process. Make sure that you make your key 4096 bits, GitHub won\u0026rsquo;t accept anything less. (I believe that all gpg commands can be substituted for gpg2 from this point onwards but I haven\u0026rsquo;t fully tested that so YMMV)\n2 - Export the Key Now we need to export the key in a format that we can give to GitHub.\ngpg --list-secret-keys --keyid-format LONG will give you a list of your secret keys, copy the ID in the same place as the red box in the screenshot and run the gpg --armor --export YOURIDHERE to get the GitHubbable part. Copy this key, all the way from -----BEGIN PGP PUBLIC KEY BLOCK----- to -----END PGP PUBLIC KEY BLOCK-----\n3 - Upload Key to GitHub This is assuming that you\u0026rsquo;re using GitHub here, you would have to upload this in basically the same format to BitBucket or wherever else.\nHead to https://github.com/settings/keys and locate the GPG keys section. Click New GPG key.\nPaste in the exported key from the previous command into the box and click Add GPG key once again.\nVerify that the key is now showing in the GPG keys list and has the correct email address associated with it.\n4 - Complete Git Config Check that you don\u0026rsquo;t already have a .gitconfig file using cat. If you do, you can probably skip some of these config steps. The last 2 are the key ones here. Use the same Key ID and for the signing key that you used in the export command from step 2.\ngit config --global user.name \u0026#34;your name\u0026#34; git config --global user.email your@email.address git config --global user.signingkey YOURIDHERE git config --global commit.gpgsign true 5 - Do All The git Things Commit, Commit, Commit! You can verify the commit using git verify-commit commitid\n","permalink":"https://dangerous.tech/signing-your-git-commits-by-default/","summary":"\u003ch2 id=\"managing-gitconfig-for-fun-and-profit\"\u003eManaging .gitconfig For Fun And Profit\u003c/h2\u003e\n\u003cp\u003eYou know when you see that little green \u0026lsquo;Verified\u0026rsquo; badge next to your commits on GitHub? The one that makes it feel all official? Notice how when you commit and push from the command line you don\u0026rsquo;t get that by default? If you\u0026rsquo;ve ever wondered why, this post is for you! \u003cem\u003eAlso if you realise that signing your commits is a good idea generally because something something security\u0026hellip; It works for you too.\u003c/em\u003e\u003c/p\u003e","title":"Signing Your Git Commits by Default"},{"content":"Pan-dev-mic? So, assuming that you\u0026rsquo;re living in the world right now you\u0026rsquo;re probably either working from home, furloughed or have been laid off. My condolences to those not in the first category, although financial troubles aside, enjoy your increased free time. I\u0026rsquo;m in the 100% working from home category, which means I have all of my Raspberry Pis, microcontrollers, PC builds and such laying around me but I have my work laptop hooked up to the main I/O in this room. Short of buying a high quality KVM that supports all the I/O that I would want to be able to switch between them quickly (shoutout Level1Techs store if you\u0026rsquo;re in the market), I don\u0026rsquo;t really want to have 3 laptops open at once to be able to access all the different dev environments I need for testing (I\u0026rsquo;ve definitely done that in the past though\u0026hellip;).\nThe Solution Enter VS Code. I\u0026rsquo;ve used VS Code as my main IDE for basically everything for over a year now. It\u0026rsquo;s a brilliant environment for editing any kind of code, there\u0026rsquo;s always tonnes of plugins and the application itself is stable yet has frequent updates. They also seem to listen to what the dev community really wants as added features and (I hope you\u0026rsquo;re sitting down) actually add them!\nI\u0026rsquo;d heard about the Remote Dev features on a few podcasts a while back but I\u0026rsquo;d never really had the need to try them out. I\u0026rsquo;m always working on Linux machines so it\u0026rsquo;s pretty easy to spin up a venv or a container and test my code. But these days, on the work laptop, I\u0026rsquo;m chained to Windows! No WSL either, the higher ups think that\u0026rsquo;s the devil or something so we\u0026rsquo;re stuck with Billy Basic Winblows 10. The saving grace? OpenSSH being included with Win10 since 1709 or something like that. This means that anything we can SSH to, we can develop on!\nPrerequisites Obviously make sure you\u0026rsquo;ve got VS Code installed. It\u0026rsquo;s code now on the Arch repos, vscode in Chocolatey and it\u0026rsquo;s also available as a --classic snap (as code) wherever snaps are sold. Make sure you can SSH to whatever device you\u0026rsquo;re trying to develop on. Most corporate VPNs have breakouts for 192.168.0.0/16 at least, with some opting for 172 or 10 range split tunnels depending on what the corporate ifrastructure is running wherever you are, so you may very well be able to get to your device with no troubles. The credentials for a user on the dev device (obviously?). 1 - Find the Extension(s) VS Code has 3 remote dev extensions:\nSSH Containers WSL Obviously you can just choose whichever one you like from those 3, or they have the extension, pictured above, which is basically a meta package and will install all 3. I did notice that when I went to install the meta packadge, SSH and Containers installed fine but WSL hung for quite some time and I wound up having to reload the window. I\u0026rsquo;d say if you don\u0026rsquo;t have WSL activated, don\u0026rsquo;t bother trying to install that one. Install whichever are relevant, we\u0026rsquo;ll be going through SSH configurations specifically in this post.\n2 - Add A New SSH Connection Pt 1 Open the command palette (F1 or Ctrl + Shift + P) and search for remote-ssh. You\u0026rsquo;ll want to select Add New SSH Host....\n3 - Add A New SSH Connection Pt 2 Now, as the example says, go ahead and input the command required to connect to the server. Just ssh user@domain should be enough here. I haven\u0026rsquo;t tried it but I assume that you could use Mosh here if you didn\u0026rsquo;t have a particularly stable connection to the remote host.\n4 - Add A New SSH Connection Pt 3 Now you\u0026rsquo;ll want to select which SSH Config file to update. Either is probably fine, I tend to go with the former because it\u0026rsquo;s user specific. You\u0026rsquo;ll definitely want the first option if you\u0026rsquo;re on a multi-user machine.\n5 - Add A New SSH Connection Pt 4 You\u0026rsquo;ll now notice this little toasty boi in the bottom right of your screen. If you want to use SSH keys to access your remote machine, click \u0026lsquo;Open Config\u0026rsquo; and continue with the optional steps. If not, skip em and head to step 9, just enter the password each time you connect.\n6 - Open The SSH Config File (OPTIONAL) Looks like any old SSH Config file, nothing to see here.\n7 - Get You An SSH Key, Son (OPTIONAL) With the magic of OpenSSH, you should have the ssh-keygen command available to you. Use powershell (or cmd if you\u0026rsquo;re a peasant\u0026hellip;) to generate an SSH key using ssh-keygen -t rsa -C key_comment. Follow the prompts, set a passphrase if you like (I left mine because this is only a Raspberry Pi and I don\u0026rsquo;t really care if I lose this key to it) and specificy a save location and you\u0026rsquo;re good to go.\n8 - Add Your SSH Key To The SSH Config File (OPTIONAL) Hope you remembererd where you put that SSH key, you\u0026rsquo;re going to need that path right about now. Add a line to the SSH Config file as in the screenshot, just needs to be IdentityFile path/to/key/file. You\u0026rsquo;ll notice I also changed the Host portion of mine, you can change this to whatever you like, just keep the HostName address the same.\n9 - Begin The Remote Connection Finally, right? All this prep and now you can finally initiate the connection! Head to the Remote Explorer and you should see the connection you\u0026rsquo;ve just added (displayed based on that Host line in the config).\n10 - Confirm The Remote OS Apparently sometimes VS Code will autodetect this, but I had to confirm it every time I\u0026rsquo;ve used this feature so far so who knows\u0026hellip; Just select the right platform, hit enter and you\u0026rsquo;ll be on your way.\n11 - Confirm The Remote Fingerprint Unless you\u0026rsquo;ve already SSH\u0026rsquo;ed to this host and have it\u0026rsquo;s details in your known_hosts file, you\u0026rsquo;ll need to confirm it now.\n12 - Enter Your Password Now is the time. SSH key users, this is the last time you\u0026rsquo;ll have to enter a password to connect (unless you added one to your SSH key but I can\u0026rsquo;t stop you there, I\u0026rsquo;m not your dad).\n13 - The Install Begins It turns out that VS Code dumps a bunch of nonsense on the remote machine in order to make the remote dev environment work. If you open the terminal now, you\u0026rsquo;ll be able to see if doing it\u0026rsquo;s work.\n14 - Open The Home Folder (OPTIONAL) Another one for you SSH key users. I\u0026rsquo;m assuming here that you haven\u0026rsquo;t already added this key to the authorized_keys file on the remote machine. If you have skip to step 19. If not, click \u0026lsquo;Open Folder\u0026rsquo; and open the user\u0026rsquo;s Home dir (or their .ssh dir if they already have one).\n15 - Create The .ssh Directory (OPTIONAL) This one\u0026rsquo;s not rocket science, create a new folder called .ssh within the Home dir if it doesn\u0026rsquo;t already exist.\n16 - Create The authorized_keys File (OPTIONAL) Now create a file called authorized_keys (again, if it doesn\u0026rsquo;t already exist - if it does, just edit it).\n17 - Copy Ze Public Key (OPTIONAL) Echo out the public part of the key that you created earlier (type filename on windows, cat filename on *nix).\n18 - Paste Ze Public Key (OPTIONAL) Paste that key into the authorized_keys file. Just the bit beginning with ssh-rsa and ending with whatever your comment was from when you created the key.\n19 - Close The Remote Connection (OPTIONAL) Now, close the remote connection. We\u0026rsquo;ll reopen it using our key to authenticate, and you don\u0026rsquo;t get the instanst gratification of seeing that in action unless you close the window. Ashes to ashes, dust to dust, tis the circle of life, my friends.\n20 - Remove The Folder From VS Code (OPTIONAL) VS Code remembers the home folder you were just in but unless you run your code straight out of there (which you don\u0026rsquo;t\u0026hellip;\u0026hellip; do you\u0026hellip;.?) then you don\u0026rsquo;t need this in the history.\n21 - To Open Or To Clone? (OPTIONAL) Now, relaunch the connection the same way you did in step 9, except this time you won\u0026rsquo;t have to do any of the additional nonsense. You should connect straight in and be greeted with the File Browser. Either open a folder which already has a coding project in or clone one, the decision\u0026rsquo;s all yours.\n22 - A Point About Extensions You\u0026rsquo;ll now notice if you open the extensions sidebar you have a section for local extensions and one for your remote machine. Any extensions that you need for debugging, building, executing, etc need to be installed on the remote machine. Anything that just syntax highlighting, linting, etc will work just fine on the local machine. Every time you install en extension while you\u0026rsquo;re connected to a remote host, it will install it both on the local machine and the remote by default. If you select the extension to bring up the page properly, you get the option to just install on the remote, should you wish.\nI always like to have GitLens on the remote machine because then it can keep the state of the repo on the remote machine in check. See the post about setting up your .gitconfig(I\u0026rsquo;ll link this when it\u0026rsquo;s live) for more information on Git config in general.\n23 - Final Interface Walkthrough I\u0026rsquo;ll leave you with a few pointers on the final interface. Hack the planet!\n","permalink":"https://dangerous.tech/remote-development-with-vs-code/","summary":"\u003ch2 id=\"pan-dev-mic\"\u003ePan-dev-mic?\u003c/h2\u003e\n\u003cp\u003eSo, assuming that you\u0026rsquo;re living in the world right now you\u0026rsquo;re probably either working from home, furloughed or have been laid off. My condolences to those not in the first category, although financial troubles aside, enjoy your increased free time. I\u0026rsquo;m in the 100% working from home category, which means I have all of my Raspberry Pis, microcontrollers, PC builds and such laying around me but I have my work laptop hooked up to the main I/O in this room. Short of buying a high quality KVM that supports all the I/O that I would want to be able to switch between them quickly (shoutout \u003ca href=\"https://store.level1techs.com/products/kvm-switch-2-port-dual-monitor-8k-model\"\u003eLevel1Techs store\u003c/a\u003e if you\u0026rsquo;re in the market), I don\u0026rsquo;t really want to have 3 laptops open at once to be able to access all the different dev environments I need for testing (I\u0026rsquo;ve definitely done that in the past though\u0026hellip;).\u003c/p\u003e","title":"Remote Development with VS Code"},{"content":"The Problem Let\u0026rsquo;s say you have a container, say for example that it\u0026rsquo;s a container you rely on to store and sync a lot of your data. Say that one day, you notice that all of your devices stop syncing and throw time out errors. Say that you\u0026rsquo;re quite frustrated with this because to a certain extent, this container stack was supposed to be set and forget. Say that you go to the web GUI and you see this:\nWhy could this be? You haven\u0026rsquo;t changed anything in this stack for a few weeks and it\u0026rsquo;s been running smoothly up until now\u0026hellip;\nLet\u0026rsquo;s take a look at the logs (docker-compose logs -f):\n(if you don\u0026rsquo;t have debug turned on, you\u0026rsquo;ll need it for this step - enable it and restart the container, then reload the web GUI)\nThe key parts to look for here are the Host field, which should contain the URL of the container that\u0026rsquo;s causing problems, and the ForwardURL field, which is where Traefik is trying to route your\u0026hellip; traffic. I\u0026rsquo;ve whited out certain sections to protect the innocent but you can still see the key bits. Most interestingly is that when I was bringing up this container stack a few weeks ago, I noted it\u0026rsquo;s 192.168 address range. You\u0026rsquo;ll see from the screenshot that the address Traefik is trying to send to is a 172.27 address\u0026hellip;\u0026hellip;. If you\u0026rsquo;ve ever spent any time with IP addresses, you\u0026rsquo;ll know that this is rather incorrect, and not just incorrect due to some sort of chance act or stale cache. Let\u0026rsquo;s investigate further - docker container inspect the-problem-container:\nYou\u0026rsquo;ll see the problem quite clearly here in the IPAddress field - Traefik is trying to route to the _backend network, which only the internals of the container stack have access to because\u0026hellip; security. Because of that, Traefik is receiving timeouts as the interface that it has on the traefik_proxy network isn\u0026rsquo;t allowed in to the _backend network.\nThe Solution Now that we\u0026rsquo;ve identified the problem, how do we fix it? We tell Traefik exactly which network it should be detecting the container on, of course! How do we do this? With a label, just like all of our other Traefik config.\n- \u0026quot;traefik.docker.network=traefik_proxy\u0026quot;\nSimply add this label to the problem container, recreate both containers for safety (start with the problem one, do Traefik last - and if you\u0026rsquo;ve still got debug turned on you can turn it off, it\u0026rsquo;s quite noisy otherwise) and you should be good to go. Check the web GUI and verify that you\u0026rsquo;re all back up to speed. Now go and add this label to every other container you\u0026rsquo;re using Traefik to route to just in case and you\u0026rsquo;ll set your mind at ease.\nAddendum You don\u0026rsquo;t necessarily have to add it to all of your containers, I\u0026rsquo;ve only ever seen issues with containers that have multiple networks attached. So just go and add it to the relevant ones.\nSOTP: Who Got It? - ATTYA Super chill electronic music, great to work to.\n","permalink":"https://dangerous.tech/docker-routing-incorrect-ip-address-detected-by-traefik/","summary":"\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s say you have a container, say for example that it\u0026rsquo;s a container you rely on to store and sync a lot of your data. Say that one day, you notice that all of your devices stop syncing and throw time out errors. Say that you\u0026rsquo;re quite frustrated with this because to a certain extent, this container stack was supposed to be set and forget. Say that you go to the web GUI and you see this:\u003c/p\u003e","title":"Docker Routing - Incorrect IP Address Detected by Traefik"},{"content":"Twitter? Garbage. Facebook? Garbage. Do you enjoy having your data harvested and sold? No? Good. Back in the day, there was this thing called Internet Relay Chat. What if I told you\u0026hellip; It still exists! Internet socializing without all the Silicon Valley data theft! But due to how the ecosystem works, any time you\u0026rsquo;re not connected to the server, you don\u0026rsquo;t receive any messages. This means that you don\u0026rsquo;t get to see conversations that happen while you\u0026rsquo;re offline, even if somebody mentions you!\nEnter the IRC Bouncer. This nifty little piece of software sits behind the scenes and stays connected to the IRC server(s) whilst you\u0026rsquo;re sleeping, working or doing other things that happen IRL. Then when you return to where you\u0026rsquo;re supposed to be (i.e. in front of a computer) you reconnect and the bouncer greets you with a flood of glorious, nerdy conversations.\nAssumptions You know why HTTPS is necessary and will use it without question You understand the general working of containers and have docker installed on your system. (Docker/Docker Compose knowledge is a plus but not required, you can basically just copy/paste code to get this working. I\u0026rsquo;ll also go over some common pitfalls at the end) You have a domain or subdomain to use and it has an A record pointed at the public IP of the machine you\u0026rsquo;re doing this install on Your domain/subdomain has a CAA record that supports LetsEncrypt You Will Need A linux environment - or WSL I guess, if you hate yourself\u0026hellip; Docker Docker-compose An A record pointed at your environment\u0026rsquo;s public IP Your caffeinated beverage of choice 1 - Checking Your Install See Step 1 from this post for further version instructions. The short of it is that I\u0026rsquo;m using docker 19.03.1 and docker-compose 1.21.0.\n2 - Mozart, You Ain\u0026rsquo;t As per usual, we\u0026rsquo;re using the reverse proxy for this component so if you\u0026rsquo;re not using that, you can either ignore those particular parts or you can set that sucker up using the instructions in this post. The config is also on GitHub.\nBefore we compose all the things, we first have to generate the base ZNC config. If you\u0026rsquo;re migrating from an old install, just migrate the whole znc folder to /var/lib/docker/volumes/YOURVOLUMENAME/_data/.\n2.1 - Generate Config Run docker run -it -v znc_znc_data:/znc-data znc --makeconf then follow the process through like the below (you should customize your username, nicks, ident and bind host though).\n[ .. ] Checking for list of available modules... [ ** ] [ ** ] -- Global settings -- [ ** ] [ ?? ] Listen on port (1025 to 65534): 6697 [ ?? ] Listen using SSL (yes/no) [no]: yes [ .. ] Verifying the listener... [ ** ] Unable to locate pem file: [/znc-data/znc.pem], creating it [ .. ] Writing Pem file [/znc-data/znc.pem]... [ ** ] Enabled global modules [webadmin] [ ** ] [ ** ] -- Admin user settings -- [ ** ] [ ?? ] Username (alphanumeric): yournamehere [ ?? ] Enter password: [ ?? ] Confirm password: [ ?? ] Nick [yournamehere]: [ ?? ] Alternate nick [yournamehere_]: [ ?? ] Ident [yournamehere]: [ ?? ] Real name (optional): [ ?? ] Bind host (optional): sub.domain.com [ ** ] Enabled user modules [chansaver, controlpanel] [ ** ] [ ?? ] Set up a network? (yes/no) [yes]: no [ ** ] [ .. ] Writing config [/znc-data/configs/znc.conf]... [ ** ] [ ** ] To connect to this ZNC you need to connect to it as your IRC server [ ** ] using the port that you supplied. You have to supply your login info [ ** ] as the IRC server password like this: user/network:pass. [ ** ] [ ** ] Try something like this in your IRC client... [ ** ] /server \u0026lt;znc_server_ip\u0026gt; +6697 yournamehere:\u0026lt;pass\u0026gt; [ ** ] [ ** ] To manage settings, users and networks, point your web browser to [ ** ] https://\u0026lt;znc_server_ip\u0026gt;:6697/ [ ** ] [ ?? ] Launch ZNC now? (yes/no) [yes]: no 2.2 - Concerto in D Major Now we need to bring up the container to get our proxy on. As per the normal process, create the docker-compose.yml file and run docker-compose up -d for the magic to get underway.\nThe only thing to mention here is the need for network_mode: \u0026quot;bridge\u0026quot; so that your ZNC container can see the proxy and vice versa. The rest is all fairly standard. If you\u0026rsquo;re not using the proxy, feel free to remove the environment variables as they\u0026rsquo;re unnecessary without it.\nIf you\u0026rsquo;re monitoring the logs of the proxy, you\u0026rsquo;ll see it pick up the new container event and obtain the SSL certificates, if not, give it 30 seconds or so and then open a web browser to the domain you\u0026rsquo;ve used.\nThe full docker-compose.yml file:\nversion: \u0026#39;3\u0026#39; services: znc: container_name: znc image: znc:latest restart: unless-stopped environment: - VIRTUAL_HOST:sub.domain.com - VIRTUAL_PORT:6697 - LETSENCRYPT_HOST:sub.domain.com volumes: - znc_data:/znc-data network_mode: \u0026#34;bridge\u0026#34; volumes: znc_data: ","permalink":"https://dangerous.tech/znc--docker-a-containerized-irc-bouncer/","summary":"\u003cp\u003eTwitter? Garbage. Facebook? Garbage. Do you enjoy having your data harvested and sold? No? Good. Back in the day, there was this thing called Internet Relay Chat. What if I told you\u0026hellip; It still exists! Internet socializing without all the Silicon Valley data theft! But due to how the ecosystem works, any time you\u0026rsquo;re not connected to the server, you don\u0026rsquo;t receive any messages. This means that you don\u0026rsquo;t get to see conversations that happen while you\u0026rsquo;re offline, even if somebody mentions you!\u003c/p\u003e","title":"ZNC + Docker - A Containerized IRC Bouncer"},{"content":"We have recently begun a project to replace all of our core switches at our various remote locations. We have around 40 locations and 2 core switches for each location so, whilst not the largest undertaking, it\u0026rsquo;s an operationally significant project for a team of 1\u0026hellip; We\u0026rsquo;ve used HP switches for a long time (we\u0026rsquo;re even still using some of the 3COM switches\u0026hellip;) and we have a supplier that gets a good discount on these so I went shopping. Now that HPE have purchased Aruba, there\u0026rsquo;s some nicer models that we can get our hands on. We don\u0026rsquo;t have any stupidly high bandwidth requirements yet either so we\u0026rsquo;re not looking in the upper ranges, we usually stick to the SMB style switches (plus I have a hard time justifying the price increase with our use cases!).\nThe 2530 or 2540 is the obvious budget model if all you need is Layer 2 and some static routing. It\u0026rsquo;s the logical follow-on from the likes of the 1910/1920 of old. We\u0026rsquo;re going to cycle these into our edge when the time comes because they\u0026rsquo;ll be good workhorse devices for a few years. The 2930 model is where things get interesting for me. Also worth pointing out that the 2920 is cool but is EOL very soon so wasn\u0026rsquo;t a worthy candidate.\nThe 2930 splits into 2 models:\n2930M - Proper stacking cables, WAY higher PoE outputs, nice layer 3 features - Larger SMB Outfits - Price tag of ~Â£2.5k (48 port non-PoE) 2930F - VSF stacking, nice layer 3 features - Smaller SMB Outfits - Price Tag of ~Â£900 (48 port non-PoE) Given the fact that previously, we\u0026rsquo;ve been paying ~Â£200 for a HP 1920 switch, a jump to Â£2.5k is a bit much\u0026hellip; The benefits of jumping to ~Â£900 was justifiable, however. Part of those benefits was the ability to stack the core switches.\nStacking is something that I toyed around with in my university days, using Cisco stacking cables to stack equipment in the lab. It was quite interesting and I could see the benefits of using it (easier switch swap out, better backplane bandwidth, etc). Coming into the industry and joining a company that not only wasn\u0026rsquo;t using Cisco equipment but was still using unmanaged 3COM switches, the reality was a little different to the fantasy. Due to this, I haven\u0026rsquo;t had the need to use any stacking for the last 3 or 4 years. When I saw the datasheet for the 2930F had VSF stacking, I started to do some research. The AirHeads forums are surpringly filled with decent information when it comes to this topic, so it didn\u0026rsquo;t take me long to figure out that I wanted to give this a shot. Queue the supplier sending me 3 2930F\u0026rsquo;s to test with.\nGetting Started At first, I didn\u0026rsquo;t see the option for VSF, which wsa a bit startling. Then I realised that I was on a firmware release from a a while back. It would appear that HPE/Aruba introduced the VSF options in firmware from January 2017 so if you need the firmware, search for your switch here and grab the latest. I went from 16.02.0003 to 16.06.0006 in one jump so there\u0026rsquo;s no interstitial upgrades required. Once the upgrade has been done to the Primary and Secondary BootROMs, you should be back up and running. Now, if you type\nshow vsf you should get the below output.\nAruba-2930F-48G-4SFP(config)# show vsf VSF is not enabled. Good, this means we can begin the config! It\u0026rsquo;s relatively simple, especially for the first switch.\nAruba-2930F-48G-4SFP(config)# vsf member 1 link 1 2 All configuration on this port has been removed and port is placed in VSF mode. Aruba-2930F-48G-4SFP(config)# vsf member 1 priority 200 Aruba-2930F-48G-4SFP(config)# vsf enable domain 254 This will save the current configuration and reboot the switch. Continue (y/n)? After hitting y at this prompt, the switch will reboot and you should be good to go. To explain some of these options further, domain is simply a number that you give to each set of stacks. Similar to VRRP or HSRP group IDs, basically. Do this part last because it\u0026rsquo;s the bit of config that triggers a reboot once comitted. The member key specifies which switch you\u0026rsquo;re operating on in the stack. We\u0026rsquo;ll see later on but as you add switches, you get members 2, 3 and so on. link is one I haven\u0026rsquo;t experimented with much yet, basically it seems that you can have 2 virtual ports on the VSF instance. Not sure what significance that has in terms of bandwidth, I\u0026rsquo;ll investigate that later. The number afterwards is simply the port number that you want to use to stack the ports together. From what I\u0026rsquo;ve research you can have multiple physical ports on one virtual link but as I mentioned, I haven\u0026rsquo;t looked much into that yet.\nPost-reboot, you\u0026rsquo;ll have a new prompt\nAruba-VSF-2930F(config)# which should indicate that VSF is live. You\u0026rsquo;ll also have an orange light on the port that you set up in the link command.\nIf you need further proof, the show command has some nice output.\nAruba-VSF-2930F(config)# sh vsf VSF Domain ID : 254 MAC Address : 941882-xxxxxx VSF Topology : No Stack Formed VSF Status : Active Uptime : 0d 0h 5m VSF MAD : None VSF Port Speed : 1G Software Version : WC.16.06.0006 Mbr ID MAC Address Model Pri Status --- ----------------- ------------------------------------- --- --------------- *1 941882-xxxxxx Aruba JL260A 2930F-48G-4SFP Switch 200 Commander This confirms the Member ID, Priority and Status of the switch. I\u0026rsquo;d recommend changing the hostname at this point to reflect that you\u0026rsquo;re going to be stacking things, otherwise it may get confusing in the future.\nAruba-VSF-2930F(config)# hostname \u0026#34;VSF-Test\u0026#34; VSF-Test(config)# Adding A Switch There\u0026rsquo;s more than one way to skin a cat when it comes to adding a switch to a VSF fabric. If you have a switch of the same model/firmware as your current commander (not sure how strict that is, see HPE/Aruba for more details), you can just plug the new one into the link port! The new switch will get the VSF config and reboot, then it will join the VSF fabric!\nAruba-2930F-48G-4SFP# Rebooting to join VSF fabric with domain ID 254 Once rebooted, you will be greeted with the commander\u0026rsquo;s login prompt.\nVSF-Test(config)# Again, the show command will confirm this.\nVSF-Test(config)# sh vsf VSF Domain ID : 254 MAC Address : 941882-xxxxxx VSF Topology : Chain VSF Status : Active Uptime : 0d 0h 17m VSF MAD : None VSF Port Speed : 1G Software Version : WC.16.06.0006 Mbr ID MAC Address Model Pri Status --- ----------------- ------------------------------------- --- --------------- 1 941882-xxxxxx Aruba JL260A 2930F-48G-4SFP Switch 200 Commander *2 e0071b-xxxxxx Aruba JL260A 2930F-48G-4SFP Switch 128 Standby There\u0026rsquo;s another couple of interesting things about this screen that you\u0026rsquo;ll notice. We have a switch with ID 2. The star indicates which switch you\u0026rsquo;re consoled in to, if you are indeed consoled in. You can also see that by default, members join the stack with a priority of 128. We\u0026rsquo;ll change this shortly. You can also see that the VSF topology is set to chain by default. You can have either chain or ring topologies for your stacking. If you\u0026rsquo;ve done any stacking before, you\u0026rsquo;ll be familiar with the concept but if not, chain is just like daisy chaining your switches together whereas ring gives you the ability to have 3 or more switches set up in such a way that if one of the middle switches fails, you\u0026rsquo;ll still be able to use all of the other switches. In a chain you would lose every switch in the chain beyond the failed switch.\nWe\u0026rsquo;d better change that priority as well, because it\u0026rsquo;s usually better to have control of these things.\nVSF-Test(config)# vsf member 2 priority 190 Replacing a Stack Member An interesting scenario that I had was replacing a stack member. As the operational overhead of configuring a new switch was one of the reasons for looking at stacking in the first place, this was a fairly key test. If we look at the running config of our VSF fabric now we get the following.\nhostname \u0026#34;VSF-Test\u0026#34; vsf enable domain 254 member 1 type \u0026#34;JL260A\u0026#34; mac-address 941882-xxxxxx priority 200 link 1 1/2 link 1 name \u0026#34;I-Link1_1\u0026#34; link 2 name \u0026#34;I-Link1_2\u0026#34; exit member 2 type \u0026#34;JL260A\u0026#34; mac-address e0071b-xxxxxx priority 190 link 1 2/2 link 1 name \u0026#34;I-Link2_1\u0026#34; link 2 name \u0026#34;I-Link2_2\u0026#34; exit port-speed 1g Besides the link 1 and 2 thing which irks me (no vsf member 1 link 2 to remove that by the way), we can see that each member appears to be bound by a MAC address. This posed an interesting question as to what would happen if we swap out a failed stack member with a new switch. Let\u0026rsquo;s find out!\nFirstly, we\u0026rsquo;re greeted by the orange light over the VSF port again. We also get an orange heatbeat symbol!\nThe switch is clearly quite sad that we\u0026rsquo;ve unplugged it\u0026rsquo;s stack buddy. If we check the logging with sh logging -r we can see some more info.\nI 01/01/90 00:36:35 04992 vsf: ST1-CMDR: VSF port 1/2 is down I 01/01/90 00:36:35 03271 stacking: ST1-CMDR: Topology is a Standalone I 01/01/90 00:36:35 03272 stacking: ST1-CMDR: Stack fragment active W 01/01/90 00:36:35 03258 stacking: ST1-CMDR: Standby switch with Member ID 2 removed due to loss of communication I 01/01/90 00:36:35 04992 vsf: ST1-CMDR: VSF link 1 is down I 01/01/90 00:36:35 04992 vsf: ST1-CMDR: VSF port 1/2 is in error state Luckily, I\u0026rsquo;ve got a 3rd switch ready to jump into the fray! After plugging the new one into the same port and waiting for it to reboot, we get some more log info.\nI 01/01/90 00:43:11 00539 stacking: ST1-CMDR: Initial sync to standby starting I 01/01/90 00:43:10 00256 ports: ST1-CMDR: Port 3/2 is reserved for VSF use W 01/01/90 00:43:10 03270 stacking: ST1-CMDR: Topology is a Chain I 01/01/90 00:43:10 02555 chassis: ST1-CMDR: Co-processor Ready I 01/01/90 00:43:08 03125 mgr: ST1-CMDR: Startup configuration changed by SNMP. New seq. number 5 I 01/01/90 00:43:08 03803 chassis: ST1-CMDR: System Self test completed on 3/1-52 W 01/01/90 00:43:08 03270 stacking: ST1-CMDR: Topology is a Chain I 01/01/90 00:43:07 03279 stacking: ST1-CMDR: Member 3 (e0071b-xxxxyy) chosen as standby. Reason: Only available standby I 01/01/90 00:43:07 04992 vsf: ST1-CMDR: VSF link 1 is up I 01/01/90 00:43:02 04987 vsf: ST1-CMDR: VSF link 1 up: Peer has mac e0071b-xxxxyy I 01/01/90 00:43:02 04988 vsf: ST1-CMDR: VSF link 1 port 1/2 up: Peer validated Looks like we\u0026rsquo;re syncing the initial config which is always nice. The major problem that I can see here is that Port 3/2 is reserved for VSF use. Now, that first number indicates the chassis number, in VSF that would be the same as our member ID. Previously, we only had 1 and 2, now we appear to have a 3rd\u0026hellip; Looking at the show command, we get the bad news. The eagle eyed amongst you will also notice that I\u0026rsquo;m masking the MAC address with y\u0026rsquo;s for the 3rd switch. This will continue going forwards so that we know which switch is which.\nVSF-Test(config)# sh vsf VSF Domain ID : 254 MAC Address : 941882-xxxxxx VSF Topology : Chain VSF Status : Fragment Active Uptime : 0d 0h 46m VSF MAD : None VSF Port Speed : 1G Software Version : WC.16.06.0006 Mbr ID MAC Address Model Pri Status --- ----------------- ------------------------------------- --- --------------- *1 941882-xxxxxx Aruba JL260A 2930F-48G-4SFP Switch 200 Commander 2 e0071b-xxxxxx Aruba JL260A 2930F-48G-4SFP Switch 190 Missing 3 e0071b-xxxxyy Aruba JL260A 2930F-48G-4SFP Switch 128 Standby Hmm\u0026hellip; If you remember the running config output from earlier, you\u0026rsquo;ll remember that the MAC address was set as a command. Let\u0026rsquo;s give that a try.\nVSF-Test(config)# vsf member 2 type \u0026#34;JL260A\u0026#34; mac-address e0071b-xxxxyy This will save the current configuration. Continue (y/n)? y A switch with the specified MAC address already exists. Oops, no joy there. The list of options available for the VSF member isn\u0026rsquo;t extensive either.\nVSF-Test(config)# vsf member 3 link Create an VSF link on the specified member. priority Assign a priority to the specified VSF virtual chassis member. remove Erase the VSF virtual chassis member configuration. shutdown Shut down the VSF virtual chassis member. type Configure the family of the VSF member-switch being provisioned. Now, as we\u0026rsquo;re replacing a member that should have configuration on, we can\u0026rsquo;t just remove member 2 and re-add it. Here\u0026rsquo;s the cleanest way I\u0026rsquo;ve found to do this (pro tip: for future break fixes, don\u0026rsquo;t get this far. Have the break/fix engineer swapping out your kit give you the MAC of the new switch before plugging it in, that way you can do this without the extra steps).\nRemove the new member with vsf member 3 remove. You\u0026rsquo;ll get a complaint from the switch but such is life.\nThe specified VSF virtual chassis standby member will be removed and its configuration will be erased. The resulting configuration will be saved. The VSF standby member will be shutdown. Continue (y/n)? y Now you should be able to run the vsf member 2 type \u0026quot;JL260A\u0026quot; mac-address e0071b-xxxxyy command that failed earlier. Your status will show as provisioned in sh vsf.\nID MAC Address Model Pri Status --- ----------------- ------------------------------------- --- --------------- *1 941882-xxxxxx Aruba JL260A 2930F-48G-4SFP Switch 200 Commander 2 e0071b-xxxxyy Aruba JL260A 2930F-48G-4SFP Switch 190 Provisioned Have a kind human (or break/fix engineer, whatever is available) hard reboot the new switch. Not ideal, I\u0026rsquo;m aware. Short of separating the 2, manually connecting to the new switch and factory resetting it, this is the best way I\u0026rsquo;ve found.\nWelcome your new stack member into the fold!\nI 01/01/90 01:04:41 00539 stacking: ST1-CMDR: Initial sync to standby starting I 01/01/90 01:04:40 00256 ports: ST1-CMDR: Port 2/2 is reserved for VSF use W 01/01/90 01:04:40 03270 stacking: ST1-CMDR: Topology is a Chain I 01/01/90 01:04:40 02555 chassis: ST1-CMDR: Co-processor Ready I 01/01/90 01:04:38 03803 chassis: ST1-CMDR: System Self test completed on 2/1-52 W 01/01/90 01:04:38 03270 stacking: ST1-CMDR: Topology is a Chain I 01/01/90 01:04:38 03125 mgr: ST1-CMDR: Startup configuration changed by SNMP. New seq. number 7 I 01/01/90 01:04:38 03279 stacking: ST1-CMDR: Member 2 (e0071b-xxxxyy) chosen as standby. Reason: Only available standby I 01/01/90 01:04:38 04992 vsf: ST1-CMDR: VSF link 1 is up I 01/01/90 01:04:33 04987 vsf: ST1-CMDR: VSF link 1 up: Peer has mac e0071b-xxxxyy I 01/01/90 01:04:33 04988 vsf: ST1-CMDR: VSF link 1 port 1/2 up: Peer validated ID MAC Address Model Pri Status --- ----------------- ------------------------------------- --- --------------- *1 941882-xxxxxx Aruba JL260A 2930F-48G-4SFP Switch 200 Commander 2 e0071b-xxxxyy Aruba JL260A 2930F-48G-4SFP Switch 190 Standby Booting ID MAC Address Model Pri Status --- ----------------- ------------------------------------- --- --------------- *1 941882-xxxxxx Aruba JL260A 2930F-48G-4SFP Switch 200 Commander 2 e0071b-xxxxyy Aruba JL260A 2930F-48G-4SFP Switch 190 Standby Now you can continue on with your day, safe in the knowledge that the P1, 4 Hour Fix call you were working on is resolved. Good Job!\nWord To The Wise The smart way to do this is to get the break/fix engineer to phone you before they get to the site. Then, they can provide you with the MAC address from the side of the box, as seen below.\nThat way, you can run the vsf member 2 type \u0026quot;JL260A\u0026quot; mac-address e0071b-xxxxyy command well before there is ever the mention of a 3rd member.\n","permalink":"https://dangerous.tech/configuring-vsf-virtual-switching-framework-stacking-on-the-hpe/aruba-2930f/","summary":"\u003cp\u003eWe have recently begun a project to replace all of our core switches at our various remote locations. We have around 40 locations and 2 core switches for each location so, whilst not the largest undertaking, it\u0026rsquo;s an operationally significant project for a team of 1\u0026hellip; We\u0026rsquo;ve used HP switches for a long time (we\u0026rsquo;re even still using some of the 3COM switches\u0026hellip;) and we have a supplier that gets a good discount on these so I went shopping. Now that HPE have purchased Aruba, there\u0026rsquo;s some nicer models that we can get our hands on. We don\u0026rsquo;t have any stupidly high bandwidth requirements yet either so we\u0026rsquo;re not looking in the upper ranges, we usually stick to the SMB style switches (plus I have a hard time justifying the price increase with our use cases!).\u003c/p\u003e","title":"Configuring VSF (Virtual Switching Framework) Stacking on the HPE/Aruba 2930F"},{"content":"Tell me if this rings a bell: You run something from pip and get told that your pip version needs an upgrade. You run pip install --upgrade pip and then think about how many other packages might need an upgrade\u0026hellip; So, you run pip list --outdated and it returns a tonne of packages. You then spend a stupidly long amount of time trying to figure out why there\u0026rsquo;s no auto-upgrade option for pip which eventually results in you upgrading each package manually, cursing every time.\nEnter pip-review! The documentation is useful enough to get you through the first runs and pip-review -h will show you all of the options. I usually run it with pip-review --auto --verbose because I\u0026rsquo;m an edgy gentleman but if your fedora doesn\u0026rsquo;t tip as hard then you can run it with --interactive to get it to prompt you at each upgrade.\n","permalink":"https://dangerous.tech/the-magic-of-upgrading-pip-packages/","summary":"\u003cp\u003eTell me if this rings a bell: You run something from \u003ccode\u003epip\u003c/code\u003e and get told that your \u003ccode\u003epip\u003c/code\u003e version needs an upgrade. You run \u003ccode\u003epip install --upgrade pip\u003c/code\u003e and then think about how many other packages might need an upgrade\u0026hellip; So, you run \u003ccode\u003epip list --outdated\u003c/code\u003e and it returns a tonne of packages. You then spend a stupidly long amount of time trying to figure out why there\u0026rsquo;s no auto-upgrade option for \u003ccode\u003epip\u003c/code\u003e which eventually results in you upgrading each package manually, cursing every time.\u003c/p\u003e","title":"The Magic of Upgrading Pip Packages"},{"content":"So I had a need earlier on today. The need for simple end-user notifications. This need has some backstory, however\u0026hellip;\nThe plex container that I run on Docker is basically bulletproof. It does everything super quickly and is easy to restart for updates or to clear out some disk I/O issue with my NAS. The NAS is the only thing that\u0026rsquo;s really causing it issues because it\u0026rsquo;s a 5 year old Drobo which is in serious need of an upgrade! So if the Drobo locks up and kills all of the I/O to it, the containers need a reboot. Oddly enough, the Drobo itself doesn\u0026rsquo;t need a reboot (or even to be remounted) but that\u0026rsquo;s an investigation for another day. Here, we come to my need: to alert users when the container was \u0026lsquo;down for maintenance.\u0026rsquo;\nI\u0026rsquo;ve recently started using Discord for support issues with anything I host for friends and family (NextCloud, Plex, DokuWiki, etc) because it\u0026rsquo;s easy to make channels for each type of support request, as well as easily message either an individual user or a whole group of users at once. Every time I\u0026rsquo;m doing maintenance on the container, I have to post in the channel to let everyone know that it\u0026rsquo;s happening and that takes more than 0 effort. This clearly requires automation!\nDiscord offers WebHook functionality which I\u0026rsquo;ve found to be super useful for this purpose. WebHooks are a bit like a reverse API. The server sends data whenever it has it, as opposed to an API which the client queries on a regular basis and can either get data or get a \u0026rsquo;nope\u0026rsquo; in response. Kind of wasteful when you think about it that way. I found this video quite helpful when searching out the bare essentials. You can make a WebHook that posts to a specific channel on your server (useful for my purposes as I can have a seperate WebHook for each of the above hosted applications). I\u0026rsquo;m also an avid Python fan, therefore everything musts be automated in Python! I\u0026rsquo;m low key getting kind of into GO lately though\nLuckily, if you don\u0026rsquo;t count the shebang, it\u0026rsquo;s 4 lines of code!\n#!/usr/bin/python3 import requests discord_url = \u0026#34;https://discordapp.com/api/webhooks/\u0026lt;hookid\u0026gt;/\u0026lt;tokenid\u0026gt;\u0026#34; data = {\u0026#34;content\u0026#34;: \u0026#34;Plex container is rebooting...\u0026#34;} requests.post(discord_url, data=data) There you have it! All I\u0026rsquo;ve done with this so far is make a script that restarts the plex container and it calls this script just before reboot. I\u0026rsquo;m planning to add some more logic to it for monitoring purposes (e.g. time the container restart, see if it hangs, etc) and in the future set up a log monitor on some of the containers so that if they can\u0026rsquo;t see the NAS for whatever reason, a script fires off a couple of WebHooks posts and then restarts it for me. The ultimate in laziness automation!\n","permalink":"https://dangerous.tech/webhooks-python-discord-oh-my/","summary":"\u003cp\u003eSo I had a need earlier on today. The need for simple end-user notifications. This need has some backstory, however\u0026hellip;\u003c/p\u003e\n\u003cp\u003eThe plex container that I run on Docker is basically bulletproof. It does everything super quickly and is easy to restart for updates or to clear out some disk I/O issue with my NAS. The NAS is the only thing that\u0026rsquo;s really causing it issues because it\u0026rsquo;s a 5 year old \u003ca href=\"http://www.drobo.com/downloads/products/DS-0084-00_B800fs.pdf\"\u003eDrobo\u003c/a\u003e which is in serious need of an upgrade! So if the Drobo locks up and kills all of the I/O to it, the containers need a reboot. Oddly enough, the Drobo itself doesn\u0026rsquo;t need a reboot (or even to be remounted) but that\u0026rsquo;s an investigation for another day. Here, we come to my need: to alert users when the container was \u0026lsquo;down for maintenance.\u0026rsquo;\u003c/p\u003e","title":"WebHooks \u0026 Python \u0026 Discord! Oh My!"},{"content":"Week 2 of \u0026ldquo;Isn\u0026rsquo;t AutoIT Cool\u0026rsquo; is a brief one. More of a code snippet than the last post. It\u0026rsquo;s something that I figured out when I was looking into exfiltrating data from a target machine. The idea was this: plug a flash drive into a PC, have it automatically run an exe (this was back in the autoexec.bat days so the theory was that you\u0026rsquo;d call said exe from autoexec.bat) and copy a bunch of interesting files to itself and then prepare for removal. This was back before the days of the USB Rubber Ducky as well (I think the USB Switchblade was the new hotness at that point). I\u0026rsquo;ll eventually get into my full script for this process, as it wound up being quite useful. Nothing quite as useful as the LEDs on the Bash Bunny or anything but this was many years before the Hak5 crew had those for sale.\nThe script is fairly simple so I\u0026rsquo;ve posted it at the bottom of this post but there\u0026rsquo;s also a link to a gist should you require it.\nLine 1 executes an AutoIT built in function to iterate over all of the drives in the machine that are marked as \u0026lsquo;REMOVABLE.\u0026rsquo; This did become an issue with certain SanDisk drives that showed up in Windows as HDD-style drives but my way around that was to buy a different flash drive\u0026hellip;\nThe loop that makes up the rest of the snippet moves through that array and searches for a drive named \u0026lsquo;COPY.\u0026rsquo; You can, of course, change this to the name of your device, whatever that ends up being. When that\u0026rsquo;s found, the $drive_letter variable is set to the letter of the drive. Therefore, you can use that to point your file copies at. More on this when the full script is up.\n$drive_arr = DriveGetDrive(\u0026#34;REMOVABLE\u0026#34;) For $i = 0 to $drive_arr[0] If DriveGetLabel($drive_arr[$i]) = \u0026#34;COPY\u0026#34; Then $drive_letter = $drive_arr[$i] EndIf Next Gist for the curious\u0026hellip;\n","permalink":"https://dangerous.tech/easy-removeable-drive-detection-with-autoit/","summary":"\u003cp\u003eWeek 2 of \u0026ldquo;Isn\u0026rsquo;t AutoIT Cool\u0026rsquo; is a brief one. More of a code snippet than the last post. It\u0026rsquo;s something that I figured out when I was looking into exfiltrating data from a target machine. The idea was this: plug a flash drive into a PC, have it automatically run an exe (this was back in the autoexec.bat days so the theory was that you\u0026rsquo;d call said exe from autoexec.bat) and copy a bunch of interesting files to itself and then prepare for removal. This was back before the days of the \u003ca href=\"https://hakshop.com/products/usb-rubber-ducky-deluxe\"\u003eUSB Rubber Ducky\u003c/a\u003e as well (I think the \u003ca href=\"https://www.youtube.com/watch?v=PsgH8NbHTy8\"\u003eUSB Switchblade\u003c/a\u003e was the new hotness at that point). I\u0026rsquo;ll eventually get into my full script for this process, as it wound up being quite useful. Nothing quite as useful as the LEDs on the \u003ca href=\"https://hakshop.com/products/bash-bunny\"\u003eBash Bunny\u003c/a\u003e or anything but this was many years before the Hak5 crew had those for sale.\u003c/p\u003e","title":"Easy Removeable Drive Detection with AutoIT"},{"content":"AutoIt is amazing. They have a website. Read about it there because it would be a waste of my time to try and explain it to you. The help file is massive so anything you need, you can find there. Also, if you want to use the user created functions and such, everything\u0026rsquo;s in the help file. It really is rather extensive.\nSo, onto an example. This is basically an example of a script that you can run from an autoexec.bat on someone else\u0026rsquo;s computer to give them a fake and very obvious virus. Let\u0026rsquo;s begin:\n#include \u0026lt;GUIConstantsEx.au3\u0026gt; $loop = 0 $x = 1 You will need this first line to allow you to use the GUI controls. We will be creating the GUI as an overlay on top of everything on the screen. This will make the user think that something weird is going on with their computer because they can\u0026rsquo;t see any of their windows. We\u0026rsquo;re also initialising our variable\u0026rsquo;s here for good practise.\nWinMinimizeAll() GUICreate(\u0026#34;PWN TIME\u0026#34;, 4000, 4000) Line 1 here minimizes all of the other windows which isn\u0026rsquo;t really necessary but it\u0026rsquo;s a pretty cool action to use in some other scripts. Line 2 is obviously the GUI creation. It\u0026rsquo;s 4000 by 4000 because I\u0026rsquo;m not aware of many people that run their resolution this high so it\u0026rsquo;s likely to work with everyone but by all means, change it to your liking.\nwhile $loop \u0026lt; 20 ProcessClose(\u0026#34;taskmgr.exe\u0026#34;) GUISetState(@SW_SHOW) GUISetBkColor(0xFF0000) MouseMove($loop, $loop + 1) GUISetBkColor(0x000000) For $x = 300 to 3000 step 300 BlockInput(1) Beep($x,1) Next $loop = $loop + 1 Wend Here is the meat of the script, as well as those variables that I promised earlier. The while loop is there to control the amount of times the screen will flash. If you want an infinite loop, you can either set this to $loop = 0 (at the very bottom) and comment out the second to last line or just comment out the second to last line; it\u0026rsquo;s really up to you. Line 2 closes any attempts that the user makes at pressing ctrl + alt + del so that they think that their pc is truly locked up. Line 3 shows the GUI. Line 4 sets the colour of the GUI to red. Line 5 moves the mouse creepily. You can set this to a randomly generated number but it will only work really well on fast machines, not on the ones with crappy processors. Line 6 sets the GUI colour to black. You can change any one of the colour lines for any colour that you like, as well as add a sleep if you want one to stay on for longer.\nLine 9 is where the beep is generated. 300 to 3000 is a pretty good range for the beep frequency but feel free to play around with this as some interesting things can happen when you do. The step is basically how many tones you want in between 300 and 3000. if you do step 1 then it will play every sound which is interesting but takes a very long time. Line 9 just blocks user input every time the sound is played because if it was outside this loop and in the while loop, they would have a small window of time in which to press ctrl + alt + del which would be bad\u0026hellip; For the parameters of beep, we give it the $x variable which is being constantly changed and the time for each beep which I have at 1 for the sake of speed. Playing around with this is very interesting and should definitely be explored. Then, we have the line that you can comment out if you want an infinite loop. Magic virus funtimes.\nIf you\u0026rsquo;d like the whole script in one (including comments to explain all of the features), here\u0026rsquo;s a Gist link.\nHave fun!\n","permalink":"https://dangerous.tech/autoit-and-its-destructive-potential/","summary":"\u003cp\u003e\u003ca href=\"https://www.autoitscript.com/site/autoit/\"\u003eAutoIt\u003c/a\u003e is amazing. They have a \u003ca href=\"https://www.autoitscript.com/site/\"\u003ewebsite.\u003c/a\u003e Read about it there because it would be a waste of my time to try and explain it to you. The \u003ca href=\"https://www.autoitscript.com/autoit3/docs/\"\u003ehelp file\u003c/a\u003e is massive so anything you need, you can find there. Also, if you want to use the user created functions and such, everything\u0026rsquo;s in the help file. It really is rather extensive.\u003c/p\u003e\n\u003cp\u003eSo, onto an example. This is basically an example of a script that you can run from an autoexec.bat on someone else\u0026rsquo;s computer to give them a fake and very obvious virus. Let\u0026rsquo;s begin:\u003c/p\u003e","title":"AutoIT and It's Destructive Potential"}]